{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem 2: Getting familiar with TensorFlow\n",
    "\n",
    "*TensorFlow* is one of the most popular deep learning framework developed by Google. If you are new to TensorFlow, please read and play with the sample in [Getting started with TensorFlow](https://www.tensorflow.org/get_started/get_started) to get started.\n",
    "\n",
    "* <b>Learning Objective:</b> In Problem 1, you implemented a fully connected network from scratch on your own. Very tedious to do it all by yourself, right? Well, we actually feel the same thing, that's why we are using tools instead of doing everything from scratch, lonely and depressed. For this part of the assignment, we will familiarize you with a widely-used deep learning framework developed by Google, TensorFlow and walk you through convolutional neural networks and show how to train them.\n",
    "* <b>Provided Codes:</b> We provide the Template class for a simple CNN model as BaseModel, predefined skeletons for conv2d() and max_pool(), as well as the dataset preprocessing parts.\n",
    "* <b>TODOs:</b> You are asked to implement the BaseModel following the detailed instrunctions and design your own model in YourModel to achieve a reasonably good performance for classification task on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Install and import libraries\n",
    "Install tensorflow and matplotlib.\n",
    "\n",
    "```\n",
    "pip install -U tensorflow matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# Add whatever you want\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"TensorFlow Version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f68ce392080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test matplotlib\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "plt.plot(x, np.maximum(0, x), label='relu')\n",
    "plt.plot(x, 1/(1 + np.exp(-x)), label='sigmoid')\n",
    "plt.plot(x, (1 - np.exp(-2 * x))/(1 + np.exp(-2 * x)), label='tanh')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.title(\"Activation functions\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.0.0\n",
      "2.000000 * 3.000000 = 6.000000\n"
     ]
    }
   ],
   "source": [
    "# Test tensorflow\n",
    "print('TensorFlow version: ' + tf.__version__)\n",
    "a = tf.constant(2.0)\n",
    "b = tf.constant(3.0)\n",
    "c = a * b\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run([a, b, c])\n",
    "print('%f * %f = %f' % (result[0], result[1], result[2]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load datasets\n",
    "Download [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) and load the dataset. In this assignment, we will use all 50,000 images for training and 10,000 images for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_training = 49000\n",
    "num_validation = 50000 - num_training\n",
    "num_test = 10000\n",
    "\n",
    "def unpickle(file):\n",
    "    import sys\n",
    "    if sys.version_info.major == 2:\n",
    "        import cPickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = cPickle.load(fo)\n",
    "        return dict['data'], dict['labels']\n",
    "    else:\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict[b'data'], dict[b'labels']\n",
    "\n",
    "def load_train_data():\n",
    "#     pass\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Load training data from cifar-10 dataset                            #\n",
    "    # Load five files from 'data_batch_1' to 'data_batch_5'                     #\n",
    "    # Reshape images and labels to the shape of [50000, 32, 32, 3]              # \n",
    "    # and [50000], respectively                                                 #\n",
    "    #############################################################################\n",
    "    import os\n",
    "    cifar10_dir = \"data/cifar-10-batches-py\"\n",
    "    all_batches = []\n",
    "    all_labels = []\n",
    "    for b in range(1,6):\n",
    "        f_train_curr = os.path.join(cifar10_dir, 'data_batch_%d' %b)\n",
    "        d_data, d_labels = unpickle(f_train_curr)\n",
    "        batch = d_data.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "#         print(batch.shape)\n",
    "        labels = np.array(d_labels)\n",
    "        all_batches.append(batch)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "    data_train = np.concatenate(all_batches)\n",
    "    labels_train = np.concatenate(all_labels)\n",
    "#     print(data_train.shape, labels_train.shape)\n",
    "    x_train = data_train[:num_training]\n",
    "    x_val = data_train[num_training:]\n",
    "    y_train = labels_train[:num_training]\n",
    "    y_val = labels_train[num_training:]\n",
    "    del batch, labels\n",
    "    return x_train, y_train, x_val, y_val\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "def load_test_data():\n",
    "#     pass\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Load testing data from cifar-10 dataset                             #\n",
    "    # Load 'test_batch' file                                                    #\n",
    "    # Reshape images and labels to the shape of [10000, 32, 32, 3]              #\n",
    "    # and [10000], respectively                                                 #\n",
    "    #############################################################################\n",
    "    import os\n",
    "    cifar10_dir = \"data/cifar-10-batches-py\"\n",
    "    f_test = os.path.join(cifar10_dir, 'test_batch')\n",
    "    d_data, d_labels = unpickle(f_test)\n",
    "    data_test = d_data.reshape(10000,3,32,32).transpose(0,2,3,1).astype('float')\n",
    "    labels_test = np.array(d_labels)\n",
    "    return data_test, labels_test\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "# Load cifar-10 data\n",
    "X_train, Y_train, X_val, Y_val = load_train_data()\n",
    "X_test, Y_test = load_test_data()\n",
    "\n",
    "# Check the shape of the dataset\n",
    "assert X_train.shape == (num_training, 32, 32, 3)\n",
    "assert Y_train.shape == (num_training, )\n",
    "assert X_val.shape == (num_validation, 32, 32, 3)\n",
    "assert Y_val.shape == (num_validation, )\n",
    "assert X_test.shape == (num_test, 32, 32, 3)\n",
    "assert Y_test.shape == (10000, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 2-1\n",
    "\n",
    "Using the code provided, implement a neural network architecture with an optimization routine according to the specification provided below.\n",
    "\n",
    "**Model:**\n",
    "- Input image with the size 32x32x3\n",
    "- 7x7 convolutional layer with 32 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- 5x5 convolutional layer with 64 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- Flatten layer (8x8x64 -> 4096)\n",
    "- Fully-connected layer with 384 output units (4096 -> 384)\n",
    "- ReLU activation layer\n",
    "- Fully-connected layer with 10 output units (384 -> 10)\n",
    "- Output logits (10)\n",
    "\n",
    "**Optimizer:**\n",
    "- Adam optimizer\n",
    "\n",
    "**Learning rate:**\n",
    "- Set start learning rate as 5e-4 and apply exponential decay every 500 steps with a base of 0.96\n",
    "- Use 'tf.train.exponential_decay' and 'tf.train.AdamOptimizer'\n",
    "\n",
    "**Loss:**\n",
    "- Softmax cross entropy loss\n",
    "- Use 'tf.nn.softmax_cross_entropy_with_logits'\n",
    "\n",
    "\n",
    "Your model **should** achieve about 60% accuracy on validation set in 5 epochs using provided evaluation code.\n",
    "\n",
    "You can modify the template code as you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define your layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define max pooling and conv layers\n",
    "def conv2d(input, kernel_size, stride, num_filter):\n",
    "    stride_shape = [1, stride, stride, 1]\n",
    "    filter_shape = [kernel_size, kernel_size, input.get_shape()[3], num_filter]\n",
    "\n",
    "    W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "    b = tf.get_variable('b', [1, 1, 1, num_filter], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.nn.conv2d(input, W, stride_shape, padding='SAME') + b\n",
    "\n",
    "def max_pool(input, kernel_size, stride):\n",
    "    ksize = [1, kernel_size, kernel_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(input, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "#############################################################################\n",
    "# TODO: You can add any layers (fully-connected, normalization)             #\n",
    "#############################################################################\n",
    "def fc(input,units):\n",
    "    return tf.contrib.layers.fully_connected(input, units,activation_fn=None)\n",
    "    \n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sample convolutional nueral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 layer: (1000, 16, 16, 32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "X = tf.convert_to_tensor(X_val,dtype=tf.float32)\n",
    "conv1 = conv2d(X, 7, 1, 32)\n",
    "relu1 = tf.nn.relu(conv1)\n",
    "pool1 = max_pool(relu1, 3, 2)            \n",
    "print('conv1 layer: ' + str(pool1.get_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 5\n",
    "        self.batch_size = 128\n",
    "        self.log_step = 50\n",
    "        self._build_model()\n",
    "\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Sample model  ' + '-' * 5)\n",
    "\n",
    "        print('intput layer: ' + str(self.X.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 7, 1, 32)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 3, 2)            \n",
    "            print('conv1 layer: ' + str(self.pool1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv2'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.conv2 = conv2d(self.pool1, 5, 1, 64)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.pool2 = max_pool(self.relu2, 3, 2)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('conv2 layer: ' + str(self.pool2.get_shape()))\n",
    "\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Flatten the output tensor from conv2 layer                          #\n",
    "        #############################################################################\n",
    "        self.flat = tf.reshape(self.pool2, [-1, 8*8*64])\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################      \n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc3'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc3 = fc(self.flat, 384)\n",
    "            self.relu3 = tf.nn.relu(self.fc3)\n",
    "            self.dropout = tf.layers.dropout(self.relu3, 1-self.keep_prob, training=self.is_train)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc3 layer: ' + str(self.relu3.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc4'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc4 = fc(self.dropout, 10)          \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc4 layer: ' + str(self.fc4.get_shape()))\n",
    "        \n",
    "        # Return the last layer\n",
    "        return self.fc4\n",
    "\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: You can add any placeholders                                        #\n",
    "        #############################################################################\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        # Adam optimizer 'self.train_op' that minimizes 'self.loss_op'\n",
    "        #############################################################################\n",
    "        # TODO: Complete the following functions                                    #\n",
    "        #############################################################################\n",
    "#         global_step = tf.Variable(0, trainable=False)\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        learning_rate = tf.train.exponential_decay(5e-4,global_step,500,0.96,staircase=True)\n",
    "        self.train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.loss_op, global_step=global_step) \n",
    "#         print(global_step)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        # Softmax cross entropy loss 'self.loss_op'\n",
    "        #############################################################################\n",
    "        # TODO: Complete the following functions                                    #\n",
    "        #############################################################################\n",
    "        self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                #############################################################################\n",
    "                # TODO: You can change feed data as you want                                #\n",
    "                #############################################################################\n",
    "                feed_dict = {self.X : X_, self.Y : Y_, self.keep_prob:0.5, self.is_train:True}              \n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "\n",
    "            #############################################################################\n",
    "            # TODO: Plot training curves                                                #\n",
    "            #############################################################################\n",
    "            # Graph 1. X: epoch, Y: training loss\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.plot(losses,'r')\n",
    "            plt.title(\"Losses\")\n",
    "            # Graph 2. X: epoch, Y: training accuracy\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.plot(accuracies,'g')\n",
    "            plt.title(\"Accuracies\")\n",
    "            plt.show()\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                        \n",
    "            #############################################################################\n",
    "            # TODO: You can change feed data as you want                                #\n",
    "            #############################################################################\n",
    "            feed_dict = {self.X: X_, self.Y : Y_, self.keep_prob:0.5, self.is_train: False}\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Sample model  -----\n",
      "intput layer: (?, 32, 32, 3)\n",
      "conv1 layer: (?, 16, 16, 32)\n",
      "conv2 layer: (?, 8, 8, 64)\n",
      "flat layer: (?, 4096)\n",
      "fc3 layer: (?, 384)\n",
      "fc4 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 20.563, accuracy = 0.094\n",
      "iteration (50): loss = 1.923, accuracy = 0.328\n",
      "iteration (100): loss = 1.805, accuracy = 0.320\n",
      "iteration (150): loss = 1.831, accuracy = 0.266\n",
      "iteration (200): loss = 1.872, accuracy = 0.266\n",
      "iteration (250): loss = 1.670, accuracy = 0.414\n",
      "iteration (300): loss = 1.512, accuracy = 0.445\n",
      "iteration (350): loss = 1.534, accuracy = 0.398\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1204a89e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.526\n",
      "train for epoch 1\n",
      "iteration (400): loss = 1.596, accuracy = 0.414\n",
      "iteration (450): loss = 1.413, accuracy = 0.477\n",
      "iteration (500): loss = 1.599, accuracy = 0.430\n",
      "iteration (550): loss = 1.215, accuracy = 0.594\n",
      "iteration (600): loss = 1.377, accuracy = 0.547\n",
      "iteration (650): loss = 1.440, accuracy = 0.484\n",
      "iteration (700): loss = 1.437, accuracy = 0.508\n",
      "iteration (750): loss = 1.152, accuracy = 0.617\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njMlxFuiNMSbHWaA3xpgc9/8B+Y8v7bujEToAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c538c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.562\n",
      "train for epoch 2\n",
      "iteration (800): loss = 1.326, accuracy = 0.508\n",
      "iteration (850): loss = 1.362, accuracy = 0.531\n",
      "iteration (900): loss = 1.220, accuracy = 0.586\n",
      "iteration (950): loss = 1.064, accuracy = 0.578\n",
      "iteration (1000): loss = 1.165, accuracy = 0.625\n",
      "iteration (1050): loss = 1.235, accuracy = 0.586\n",
      "iteration (1100): loss = 1.182, accuracy = 0.625\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12fa05d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.599\n",
      "train for epoch 3\n",
      "iteration (1150): loss = 1.151, accuracy = 0.586\n",
      "iteration (1200): loss = 0.990, accuracy = 0.641\n",
      "iteration (1250): loss = 1.185, accuracy = 0.602\n",
      "iteration (1300): loss = 1.275, accuracy = 0.555\n",
      "iteration (1350): loss = 1.051, accuracy = 0.641\n",
      "iteration (1400): loss = 1.202, accuracy = 0.633\n",
      "iteration (1450): loss = 1.108, accuracy = 0.547\n",
      "iteration (1500): loss = 1.076, accuracy = 0.648\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c54fc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.595\n",
      "train for epoch 4\n",
      "iteration (1550): loss = 0.844, accuracy = 0.742\n",
      "iteration (1600): loss = 1.045, accuracy = 0.656\n",
      "iteration (1650): loss = 1.132, accuracy = 0.617\n",
      "iteration (1700): loss = 1.036, accuracy = 0.609\n",
      "iteration (1750): loss = 1.164, accuracy = 0.594\n",
      "iteration (1800): loss = 0.996, accuracy = 0.641\n",
      "iteration (1850): loss = 1.181, accuracy = 0.602\n",
      "iteration (1900): loss = 1.028, accuracy = 0.648\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRDzpO+dcEfGk75xzReT/AyC7o7o2wt9mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1204679b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.606\n",
      "***** test accuracy: 0.609\n",
      "Model saved in lib/tf_models/problem2/csci-599_sample.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Train our sample model\n",
    "with tf.Session() as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = BaseModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_sample.ckpt\")\n",
    "        print(\"Model saved in %s\" % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 2-2\n",
    "\n",
    "Implement your own model. \n",
    "\n",
    "You can modify the template code as you want and you can use GPU for fast training.\n",
    "\n",
    "These are the techniques that you can try:\n",
    "- Data preprocessing\n",
    "- Data augmentation\n",
    "- Dropout\n",
    "- Batch normalization\n",
    "- More convolutional layers\n",
    "- More training epochs\n",
    "- Learning rate decay\n",
    "- Any other models and techniqes\n",
    "\n",
    "Your model should achieve >= 70% accuracy on the test set of CIFAR-10.\n",
    "\n",
    "If the accuracy of the model reaches to 80% on the test set, you will get 5 extra points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class YourModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        self.num_epoch = 100\n",
    "\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Your model  ' + '-' * 5)\n",
    "        \n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 5, 3, 64)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 3, 2) \n",
    "            self.norm1 = tf.nn.lrn(self.pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "            print('conv1 layer: ' + str(self.pool1.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('conv2'):\n",
    "            self.conv2 = conv2d(self.norm1, 5, 1, 64)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.norm2 = tf.nn.lrn(self.relu2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "            self.pool2 = max_pool(self.norm2, 3, 2)\n",
    "            self.dropout2 = tf.layers.dropout(self.pool2, 1-self.keep_prob, training=self.is_train)\n",
    "            print('conv2 layer: ' + str(self.pool2.get_shape()))\n",
    "\n",
    "\n",
    "        self.flat = tf.reshape(self.dropout2, [-1, 3*3*64])\n",
    "#         self.flat = tf.reshape(self.pool2, [self.batch_size,-1])\n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc3'):\n",
    "            self.fc3 = fc(self.flat, 384)\n",
    "            self.relu3 = tf.nn.relu(self.fc3)\n",
    "#             self.dropout = tf.layers.dropout(self.relu3, 1-self.keep_prob, training=self.is_train)\n",
    "            print('fc3 layer: ' + str(self.relu3.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc4'):\n",
    "            self.fc4 = fc(self.relu3, 192)     \n",
    "            self.relu4 = tf.nn.relu(self.fc4)\n",
    "            self.dropout4 = tf.layers.dropout(self.relu4, 1-self.keep_prob, training=self.is_train)\n",
    "            print('fc4 layer: ' + str(self.fc4.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('fc5'):\n",
    "            self.fc5 = fc(self.dropout4, 10)\n",
    "            print('fc5 layer: '+ str(self.fc5.get_shape()))\n",
    "        # Return the last layer\n",
    "        return self.fc5\n",
    "            \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: You can redefine BaseModel's methods                                #\n",
    "    #############################################################################\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "       \n",
    "    def _build_optimizer(self):\n",
    "        # Adam optimizer 'self.train_op' that minimizes 'self.loss_op'\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        learning_rate = tf.train.exponential_decay(5e-4,global_step,500,0.96,staircase=True)\n",
    "        self.train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.loss_op, global_step=global_step) \n",
    "#         print(global_step)\n",
    "          \n",
    "    def _loss(self, labels, logits):\n",
    "        # Softmax cross entropy loss 'self.loss_op'\n",
    "        self.loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits))\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "#         labels = tf.one_hot(self.Y, 10)\n",
    "        labels = self.Y\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                feed_dict = {self.X : X_, self.Y : Y_, self.keep_prob:0.5, self.is_train:True}              \n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "\n",
    "#             # Graph 1. X: epoch, Y: training loss\n",
    "#             plt.subplot(1,2,1)\n",
    "#             plt.plot(losses,'r')\n",
    "#             plt.title(\"Losses\")\n",
    "#             # Graph 2. X: epoch, Y: training accuracy\n",
    "#             plt.subplot(1,2,2)\n",
    "#             plt.plot(accuracies,'g')\n",
    "#             plt.title(\"Accuracies\")\n",
    "#             plt.show()\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                        \n",
    "            feed_dict = {self.X: X_, self.Y : Y_, self.keep_prob:0.5, self.is_train:False}\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Your model  -----\n",
      "conv1 layer: (?, 6, 6, 64)\n",
      "conv2 layer: (?, 3, 3, 64)\n",
      "flat layer: (?, 576)\n",
      "fc3 layer: (?, 384)\n",
      "fc4 layer: (?, 192)\n",
      "fc5 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 24.918, accuracy = 0.062\n",
      "iteration (50): loss = 2.289, accuracy = 0.102\n",
      "iteration (100): loss = 2.236, accuracy = 0.125\n",
      "iteration (150): loss = 2.257, accuracy = 0.164\n",
      "iteration (200): loss = 2.143, accuracy = 0.188\n",
      "iteration (250): loss = 2.185, accuracy = 0.133\n",
      "iteration (300): loss = 2.131, accuracy = 0.195\n",
      "iteration (350): loss = 2.105, accuracy = 0.195\n",
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.078\n",
      "train for epoch 1\n",
      "iteration (400): loss = 2.083, accuracy = 0.195\n",
      "iteration (450): loss = 2.104, accuracy = 0.188\n",
      "iteration (500): loss = 2.106, accuracy = 0.156\n",
      "iteration (550): loss = 2.085, accuracy = 0.203\n",
      "iteration (600): loss = 1.997, accuracy = 0.289\n",
      "iteration (650): loss = 1.936, accuracy = 0.172\n",
      "iteration (700): loss = 1.903, accuracy = 0.219\n",
      "iteration (750): loss = 1.881, accuracy = 0.289\n",
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.109\n",
      "train for epoch 2\n",
      "iteration (800): loss = 1.711, accuracy = 0.305\n",
      "iteration (850): loss = 1.893, accuracy = 0.211\n",
      "iteration (900): loss = 1.812, accuracy = 0.266\n",
      "iteration (950): loss = 1.801, accuracy = 0.328\n",
      "iteration (1000): loss = 1.734, accuracy = 0.391\n",
      "iteration (1050): loss = 1.730, accuracy = 0.297\n",
      "iteration (1100): loss = 1.757, accuracy = 0.312\n",
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.107\n",
      "train for epoch 3\n",
      "iteration (1150): loss = 1.889, accuracy = 0.297\n",
      "iteration (1200): loss = 1.783, accuracy = 0.312\n",
      "iteration (1250): loss = 1.838, accuracy = 0.242\n",
      "iteration (1300): loss = 1.839, accuracy = 0.258\n",
      "iteration (1350): loss = 1.635, accuracy = 0.336\n",
      "iteration (1400): loss = 1.858, accuracy = 0.305\n",
      "iteration (1450): loss = 1.672, accuracy = 0.336\n",
      "iteration (1500): loss = 1.627, accuracy = 0.352\n",
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.106\n",
      "train for epoch 4\n",
      "iteration (1550): loss = 1.630, accuracy = 0.352\n",
      "iteration (1600): loss = 1.811, accuracy = 0.344\n",
      "iteration (1650): loss = 1.579, accuracy = 0.344\n",
      "iteration (1700): loss = 1.554, accuracy = 0.359\n",
      "iteration (1750): loss = 1.670, accuracy = 0.406\n",
      "iteration (1800): loss = 1.605, accuracy = 0.344\n",
      "iteration (1850): loss = 1.763, accuracy = 0.367\n",
      "iteration (1900): loss = 1.611, accuracy = 0.359\n",
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.106\n",
      "train for epoch 5\n",
      "iteration (1950): loss = 1.661, accuracy = 0.406\n",
      "iteration (2000): loss = 1.497, accuracy = 0.367\n",
      "iteration (2050): loss = 1.665, accuracy = 0.344\n",
      "iteration (2100): loss = 1.598, accuracy = 0.414\n",
      "iteration (2150): loss = 1.631, accuracy = 0.336\n",
      "iteration (2200): loss = 1.713, accuracy = 0.336\n",
      "iteration (2250): loss = 1.531, accuracy = 0.438\n",
      "validation for epoch 5\n",
      "-  epoch 5: validation accuracy = 0.109\n",
      "train for epoch 6\n",
      "iteration (2300): loss = 1.610, accuracy = 0.414\n",
      "iteration (2350): loss = 1.541, accuracy = 0.445\n",
      "iteration (2400): loss = 1.486, accuracy = 0.414\n",
      "iteration (2450): loss = 1.625, accuracy = 0.367\n",
      "iteration (2500): loss = 1.541, accuracy = 0.375\n",
      "iteration (2550): loss = 1.460, accuracy = 0.383\n",
      "iteration (2600): loss = 1.665, accuracy = 0.422\n",
      "iteration (2650): loss = 1.534, accuracy = 0.445\n",
      "validation for epoch 6\n",
      "-  epoch 6: validation accuracy = 0.124\n",
      "train for epoch 7\n",
      "iteration (2700): loss = 1.671, accuracy = 0.430\n",
      "iteration (2750): loss = 1.519, accuracy = 0.469\n",
      "iteration (2800): loss = 1.729, accuracy = 0.367\n",
      "iteration (2850): loss = 1.418, accuracy = 0.539\n",
      "iteration (2900): loss = 1.455, accuracy = 0.438\n",
      "iteration (2950): loss = 1.454, accuracy = 0.469\n",
      "iteration (3000): loss = 1.329, accuracy = 0.477\n",
      "iteration (3050): loss = 1.395, accuracy = 0.445\n",
      "validation for epoch 7\n",
      "-  epoch 7: validation accuracy = 0.137\n",
      "train for epoch 8\n",
      "iteration (3100): loss = 1.357, accuracy = 0.469\n",
      "iteration (3150): loss = 1.555, accuracy = 0.391\n",
      "iteration (3200): loss = 1.574, accuracy = 0.414\n",
      "iteration (3250): loss = 1.382, accuracy = 0.508\n",
      "iteration (3300): loss = 1.410, accuracy = 0.500\n",
      "iteration (3350): loss = 1.604, accuracy = 0.406\n",
      "iteration (3400): loss = 1.440, accuracy = 0.500\n",
      "validation for epoch 8\n",
      "-  epoch 8: validation accuracy = 0.117\n",
      "train for epoch 9\n",
      "iteration (3450): loss = 1.459, accuracy = 0.484\n",
      "iteration (3500): loss = 1.530, accuracy = 0.555\n",
      "iteration (3550): loss = 1.574, accuracy = 0.406\n",
      "iteration (3600): loss = 1.324, accuracy = 0.570\n",
      "iteration (3650): loss = 1.475, accuracy = 0.430\n",
      "iteration (3700): loss = 1.662, accuracy = 0.461\n",
      "iteration (3750): loss = 1.450, accuracy = 0.516\n",
      "iteration (3800): loss = 1.416, accuracy = 0.453\n",
      "validation for epoch 9\n",
      "-  epoch 9: validation accuracy = 0.126\n",
      "train for epoch 10\n",
      "iteration (3850): loss = 1.347, accuracy = 0.516\n",
      "iteration (3900): loss = 1.482, accuracy = 0.539\n",
      "iteration (3950): loss = 1.412, accuracy = 0.414\n",
      "iteration (4000): loss = 1.298, accuracy = 0.508\n",
      "iteration (4050): loss = 1.415, accuracy = 0.469\n",
      "iteration (4100): loss = 1.388, accuracy = 0.508\n",
      "iteration (4150): loss = 1.433, accuracy = 0.461\n",
      "iteration (4200): loss = 1.398, accuracy = 0.555\n",
      "validation for epoch 10\n",
      "-  epoch 10: validation accuracy = 0.080\n",
      "train for epoch 11\n",
      "iteration (4250): loss = 1.464, accuracy = 0.430\n",
      "iteration (4300): loss = 1.491, accuracy = 0.438\n",
      "iteration (4350): loss = 1.289, accuracy = 0.523\n",
      "iteration (4400): loss = 1.381, accuracy = 0.453\n",
      "iteration (4450): loss = 1.536, accuracy = 0.430\n",
      "iteration (4500): loss = 1.307, accuracy = 0.516\n",
      "iteration (4550): loss = 1.429, accuracy = 0.508\n",
      "validation for epoch 11\n",
      "-  epoch 11: validation accuracy = 0.077\n",
      "train for epoch 12\n",
      "iteration (4600): loss = 1.250, accuracy = 0.570\n",
      "iteration (4650): loss = 1.496, accuracy = 0.477\n",
      "iteration (4700): loss = 1.521, accuracy = 0.484\n",
      "iteration (4750): loss = 1.330, accuracy = 0.484\n",
      "iteration (4800): loss = 1.468, accuracy = 0.461\n",
      "iteration (4850): loss = 1.161, accuracy = 0.656\n",
      "iteration (4900): loss = 1.341, accuracy = 0.539\n",
      "iteration (4950): loss = 1.340, accuracy = 0.539\n",
      "validation for epoch 12\n",
      "-  epoch 12: validation accuracy = 0.077\n",
      "train for epoch 13\n",
      "iteration (5000): loss = 1.191, accuracy = 0.555\n",
      "iteration (5050): loss = 1.364, accuracy = 0.539\n",
      "iteration (5100): loss = 1.207, accuracy = 0.602\n",
      "iteration (5150): loss = 1.300, accuracy = 0.539\n",
      "iteration (5200): loss = 1.306, accuracy = 0.531\n",
      "iteration (5250): loss = 1.335, accuracy = 0.500\n",
      "iteration (5300): loss = 1.293, accuracy = 0.484\n",
      "validation for epoch 13\n",
      "-  epoch 13: validation accuracy = 0.075\n",
      "train for epoch 14\n",
      "iteration (5350): loss = 1.331, accuracy = 0.477\n",
      "iteration (5400): loss = 1.283, accuracy = 0.531\n",
      "iteration (5450): loss = 1.435, accuracy = 0.461\n",
      "iteration (5500): loss = 1.466, accuracy = 0.477\n",
      "iteration (5550): loss = 1.314, accuracy = 0.523\n",
      "iteration (5600): loss = 1.318, accuracy = 0.516\n",
      "iteration (5650): loss = 1.296, accuracy = 0.523\n",
      "iteration (5700): loss = 1.348, accuracy = 0.461\n",
      "validation for epoch 14\n",
      "-  epoch 14: validation accuracy = 0.075\n",
      "train for epoch 15\n",
      "iteration (5750): loss = 1.191, accuracy = 0.523\n",
      "iteration (5800): loss = 1.405, accuracy = 0.484\n",
      "iteration (5850): loss = 1.256, accuracy = 0.547\n",
      "iteration (5900): loss = 1.295, accuracy = 0.516\n",
      "iteration (5950): loss = 1.229, accuracy = 0.562\n",
      "iteration (6000): loss = 1.263, accuracy = 0.531\n",
      "iteration (6050): loss = 1.386, accuracy = 0.477\n",
      "iteration (6100): loss = 1.284, accuracy = 0.594\n",
      "validation for epoch 15\n",
      "-  epoch 15: validation accuracy = 0.075\n",
      "train for epoch 16\n",
      "iteration (6150): loss = 1.409, accuracy = 0.531\n",
      "iteration (6200): loss = 1.291, accuracy = 0.523\n",
      "iteration (6250): loss = 1.408, accuracy = 0.531\n",
      "iteration (6300): loss = 1.210, accuracy = 0.500\n",
      "iteration (6350): loss = 1.211, accuracy = 0.609\n",
      "iteration (6400): loss = 1.434, accuracy = 0.477\n",
      "iteration (6450): loss = 1.280, accuracy = 0.539\n",
      "validation for epoch 16\n",
      "-  epoch 16: validation accuracy = 0.075\n",
      "train for epoch 17\n",
      "iteration (6500): loss = 1.281, accuracy = 0.562\n",
      "iteration (6550): loss = 1.115, accuracy = 0.609\n",
      "iteration (6600): loss = 1.316, accuracy = 0.531\n",
      "iteration (6650): loss = 1.361, accuracy = 0.523\n",
      "iteration (6700): loss = 1.278, accuracy = 0.531\n",
      "iteration (6750): loss = 1.335, accuracy = 0.523\n",
      "iteration (6800): loss = 1.203, accuracy = 0.594\n",
      "iteration (6850): loss = 1.214, accuracy = 0.555\n",
      "validation for epoch 17\n",
      "-  epoch 17: validation accuracy = 0.075\n",
      "train for epoch 18\n",
      "iteration (6900): loss = 1.312, accuracy = 0.492\n",
      "iteration (6950): loss = 1.120, accuracy = 0.586\n",
      "iteration (7000): loss = 1.356, accuracy = 0.523\n",
      "iteration (7050): loss = 1.295, accuracy = 0.508\n",
      "iteration (7100): loss = 1.125, accuracy = 0.609\n",
      "iteration (7150): loss = 1.226, accuracy = 0.539\n",
      "iteration (7200): loss = 1.334, accuracy = 0.523\n",
      "iteration (7250): loss = 1.226, accuracy = 0.562\n",
      "validation for epoch 18\n",
      "-  epoch 18: validation accuracy = 0.076\n",
      "train for epoch 19\n",
      "iteration (7300): loss = 1.210, accuracy = 0.562\n",
      "iteration (7350): loss = 1.338, accuracy = 0.508\n",
      "iteration (7400): loss = 1.324, accuracy = 0.531\n",
      "iteration (7450): loss = 1.094, accuracy = 0.594\n",
      "iteration (7500): loss = 1.109, accuracy = 0.602\n",
      "iteration (7550): loss = 1.215, accuracy = 0.562\n",
      "iteration (7600): loss = 1.375, accuracy = 0.500\n",
      "validation for epoch 19\n",
      "-  epoch 19: validation accuracy = 0.075\n",
      "train for epoch 20\n",
      "iteration (7650): loss = 1.110, accuracy = 0.609\n",
      "iteration (7700): loss = 1.232, accuracy = 0.562\n",
      "iteration (7750): loss = 1.348, accuracy = 0.531\n",
      "iteration (7800): loss = 1.229, accuracy = 0.562\n",
      "iteration (7850): loss = 1.212, accuracy = 0.562\n",
      "iteration (7900): loss = 1.316, accuracy = 0.539\n",
      "iteration (7950): loss = 1.319, accuracy = 0.500\n",
      "iteration (8000): loss = 1.100, accuracy = 0.648\n",
      "validation for epoch 20\n",
      "-  epoch 20: validation accuracy = 0.075\n",
      "train for epoch 21\n",
      "iteration (8050): loss = 1.162, accuracy = 0.633\n",
      "iteration (8100): loss = 1.232, accuracy = 0.523\n",
      "iteration (8150): loss = 1.188, accuracy = 0.617\n",
      "iteration (8200): loss = 1.133, accuracy = 0.586\n",
      "iteration (8250): loss = 1.242, accuracy = 0.625\n",
      "iteration (8300): loss = 1.283, accuracy = 0.547\n",
      "iteration (8350): loss = 1.134, accuracy = 0.617\n",
      "iteration (8400): loss = 1.268, accuracy = 0.547\n",
      "validation for epoch 21\n",
      "-  epoch 21: validation accuracy = 0.075\n",
      "train for epoch 22\n",
      "iteration (8450): loss = 1.073, accuracy = 0.617\n",
      "iteration (8500): loss = 1.263, accuracy = 0.539\n",
      "iteration (8550): loss = 1.089, accuracy = 0.609\n",
      "iteration (8600): loss = 1.315, accuracy = 0.508\n",
      "iteration (8650): loss = 1.101, accuracy = 0.617\n",
      "iteration (8700): loss = 1.478, accuracy = 0.500\n",
      "iteration (8750): loss = 1.257, accuracy = 0.539\n",
      "validation for epoch 22\n",
      "-  epoch 22: validation accuracy = 0.075\n",
      "train for epoch 23\n",
      "iteration (8800): loss = 1.015, accuracy = 0.633\n",
      "iteration (8850): loss = 1.208, accuracy = 0.547\n",
      "iteration (8900): loss = 1.217, accuracy = 0.531\n",
      "iteration (8950): loss = 1.022, accuracy = 0.602\n",
      "iteration (9000): loss = 1.130, accuracy = 0.555\n",
      "iteration (9050): loss = 1.262, accuracy = 0.500\n",
      "iteration (9100): loss = 1.292, accuracy = 0.547\n",
      "iteration (9150): loss = 1.116, accuracy = 0.578\n",
      "validation for epoch 23\n",
      "-  epoch 23: validation accuracy = 0.075\n",
      "train for epoch 24\n",
      "iteration (9200): loss = 1.246, accuracy = 0.602\n",
      "iteration (9250): loss = 1.118, accuracy = 0.578\n",
      "iteration (9300): loss = 0.980, accuracy = 0.617\n",
      "iteration (9350): loss = 0.979, accuracy = 0.617\n",
      "iteration (9400): loss = 1.123, accuracy = 0.547\n",
      "iteration (9450): loss = 1.186, accuracy = 0.570\n",
      "iteration (9500): loss = 1.282, accuracy = 0.539\n",
      "validation for epoch 24\n",
      "-  epoch 24: validation accuracy = 0.075\n",
      "train for epoch 25\n",
      "iteration (9550): loss = 1.207, accuracy = 0.594\n",
      "iteration (9600): loss = 1.048, accuracy = 0.641\n",
      "iteration (9650): loss = 1.187, accuracy = 0.625\n",
      "iteration (9700): loss = 1.099, accuracy = 0.625\n",
      "iteration (9750): loss = 1.200, accuracy = 0.641\n",
      "iteration (9800): loss = 1.200, accuracy = 0.594\n",
      "iteration (9850): loss = 1.086, accuracy = 0.602\n",
      "iteration (9900): loss = 1.162, accuracy = 0.609\n",
      "validation for epoch 25\n",
      "-  epoch 25: validation accuracy = 0.074\n",
      "train for epoch 26\n",
      "iteration (9950): loss = 1.087, accuracy = 0.562\n",
      "iteration (10000): loss = 1.095, accuracy = 0.562\n",
      "iteration (10050): loss = 1.215, accuracy = 0.539\n",
      "iteration (10100): loss = 1.013, accuracy = 0.633\n",
      "iteration (10150): loss = 1.152, accuracy = 0.594\n",
      "iteration (10200): loss = 1.179, accuracy = 0.555\n",
      "iteration (10250): loss = 1.157, accuracy = 0.562\n",
      "iteration (10300): loss = 1.020, accuracy = 0.656\n",
      "validation for epoch 26\n",
      "-  epoch 26: validation accuracy = 0.080\n",
      "train for epoch 27\n",
      "iteration (10350): loss = 0.992, accuracy = 0.648\n",
      "iteration (10400): loss = 1.273, accuracy = 0.516\n",
      "iteration (10450): loss = 1.091, accuracy = 0.641\n",
      "iteration (10500): loss = 1.024, accuracy = 0.586\n",
      "iteration (10550): loss = 0.966, accuracy = 0.664\n",
      "iteration (10600): loss = 1.133, accuracy = 0.586\n",
      "iteration (10650): loss = 0.960, accuracy = 0.609\n",
      "validation for epoch 27\n",
      "-  epoch 27: validation accuracy = 0.093\n",
      "train for epoch 28\n",
      "iteration (10700): loss = 1.107, accuracy = 0.609\n",
      "iteration (10750): loss = 1.027, accuracy = 0.617\n",
      "iteration (10800): loss = 1.081, accuracy = 0.594\n",
      "iteration (10850): loss = 1.243, accuracy = 0.570\n",
      "iteration (10900): loss = 0.971, accuracy = 0.602\n",
      "iteration (10950): loss = 1.198, accuracy = 0.562\n",
      "iteration (11000): loss = 1.131, accuracy = 0.633\n",
      "iteration (11050): loss = 1.023, accuracy = 0.617\n",
      "validation for epoch 28\n",
      "-  epoch 28: validation accuracy = 0.075\n",
      "train for epoch 29\n",
      "iteration (11100): loss = 0.881, accuracy = 0.695\n",
      "iteration (11150): loss = 1.074, accuracy = 0.633\n",
      "iteration (11200): loss = 1.032, accuracy = 0.609\n",
      "iteration (11250): loss = 1.129, accuracy = 0.656\n",
      "iteration (11300): loss = 1.111, accuracy = 0.570\n",
      "iteration (11350): loss = 1.115, accuracy = 0.578\n",
      "iteration (11400): loss = 1.084, accuracy = 0.602\n",
      "iteration (11450): loss = 1.099, accuracy = 0.570\n",
      "validation for epoch 29\n",
      "-  epoch 29: validation accuracy = 0.078\n",
      "train for epoch 30\n",
      "iteration (11500): loss = 1.148, accuracy = 0.539\n",
      "iteration (11550): loss = 1.004, accuracy = 0.625\n",
      "iteration (11600): loss = 1.101, accuracy = 0.672\n",
      "iteration (11650): loss = 0.926, accuracy = 0.641\n",
      "iteration (11700): loss = 1.218, accuracy = 0.570\n",
      "iteration (11750): loss = 1.129, accuracy = 0.594\n",
      "iteration (11800): loss = 1.071, accuracy = 0.617\n",
      "validation for epoch 30\n",
      "-  epoch 30: validation accuracy = 0.075\n",
      "train for epoch 31\n",
      "iteration (11850): loss = 0.985, accuracy = 0.570\n",
      "iteration (11900): loss = 1.074, accuracy = 0.609\n",
      "iteration (11950): loss = 1.030, accuracy = 0.688\n",
      "iteration (12000): loss = 1.206, accuracy = 0.586\n",
      "iteration (12050): loss = 0.997, accuracy = 0.641\n",
      "iteration (12100): loss = 1.028, accuracy = 0.625\n",
      "iteration (12150): loss = 1.033, accuracy = 0.648\n",
      "iteration (12200): loss = 1.014, accuracy = 0.609\n",
      "validation for epoch 31\n",
      "-  epoch 31: validation accuracy = 0.104\n",
      "train for epoch 32\n",
      "iteration (12250): loss = 1.201, accuracy = 0.578\n",
      "iteration (12300): loss = 1.191, accuracy = 0.539\n",
      "iteration (12350): loss = 1.030, accuracy = 0.641\n",
      "iteration (12400): loss = 1.121, accuracy = 0.703\n",
      "iteration (12450): loss = 0.933, accuracy = 0.656\n",
      "iteration (12500): loss = 1.023, accuracy = 0.633\n",
      "iteration (12550): loss = 0.974, accuracy = 0.648\n",
      "iteration (12600): loss = 1.044, accuracy = 0.625\n",
      "validation for epoch 32\n",
      "-  epoch 32: validation accuracy = 0.083\n",
      "train for epoch 33\n",
      "iteration (12650): loss = 1.085, accuracy = 0.625\n",
      "iteration (12700): loss = 1.165, accuracy = 0.570\n",
      "iteration (12750): loss = 1.078, accuracy = 0.672\n",
      "iteration (12800): loss = 1.096, accuracy = 0.578\n",
      "iteration (12850): loss = 1.097, accuracy = 0.594\n",
      "iteration (12900): loss = 1.156, accuracy = 0.562\n",
      "iteration (12950): loss = 1.079, accuracy = 0.602\n",
      "validation for epoch 33\n",
      "-  epoch 33: validation accuracy = 0.105\n",
      "train for epoch 34\n",
      "iteration (13000): loss = 1.042, accuracy = 0.617\n",
      "iteration (13050): loss = 1.038, accuracy = 0.625\n",
      "iteration (13100): loss = 1.050, accuracy = 0.609\n",
      "iteration (13150): loss = 1.026, accuracy = 0.617\n",
      "iteration (13200): loss = 1.159, accuracy = 0.594\n",
      "iteration (13250): loss = 1.243, accuracy = 0.531\n",
      "iteration (13300): loss = 0.895, accuracy = 0.703\n",
      "iteration (13350): loss = 0.966, accuracy = 0.664\n",
      "validation for epoch 34\n",
      "-  epoch 34: validation accuracy = 0.110\n",
      "train for epoch 35\n",
      "iteration (13400): loss = 1.035, accuracy = 0.648\n",
      "iteration (13450): loss = 1.062, accuracy = 0.641\n",
      "iteration (13500): loss = 1.090, accuracy = 0.562\n",
      "iteration (13550): loss = 1.049, accuracy = 0.625\n",
      "iteration (13600): loss = 0.948, accuracy = 0.656\n",
      "iteration (13650): loss = 1.172, accuracy = 0.609\n",
      "iteration (13700): loss = 0.971, accuracy = 0.688\n",
      "iteration (13750): loss = 1.019, accuracy = 0.625\n",
      "validation for epoch 35\n",
      "-  epoch 35: validation accuracy = 0.110\n",
      "train for epoch 36\n",
      "iteration (13800): loss = 0.945, accuracy = 0.625\n",
      "iteration (13850): loss = 1.165, accuracy = 0.602\n",
      "iteration (13900): loss = 0.968, accuracy = 0.609\n",
      "iteration (13950): loss = 1.068, accuracy = 0.602\n",
      "iteration (14000): loss = 1.028, accuracy = 0.648\n",
      "iteration (14050): loss = 0.938, accuracy = 0.672\n",
      "iteration (14100): loss = 1.031, accuracy = 0.633\n",
      "validation for epoch 36\n",
      "-  epoch 36: validation accuracy = 0.112\n",
      "train for epoch 37\n",
      "iteration (14150): loss = 0.862, accuracy = 0.695\n",
      "iteration (14200): loss = 1.150, accuracy = 0.617\n",
      "iteration (14250): loss = 1.100, accuracy = 0.633\n",
      "iteration (14300): loss = 1.037, accuracy = 0.625\n",
      "iteration (14350): loss = 1.019, accuracy = 0.633\n",
      "iteration (14400): loss = 1.015, accuracy = 0.633\n",
      "iteration (14450): loss = 0.933, accuracy = 0.672\n",
      "iteration (14500): loss = 0.903, accuracy = 0.680\n",
      "validation for epoch 37\n",
      "-  epoch 37: validation accuracy = 0.110\n",
      "train for epoch 38\n",
      "iteration (14550): loss = 0.877, accuracy = 0.727\n",
      "iteration (14600): loss = 0.969, accuracy = 0.641\n",
      "iteration (14650): loss = 0.937, accuracy = 0.641\n",
      "iteration (14700): loss = 0.949, accuracy = 0.672\n",
      "iteration (14750): loss = 0.971, accuracy = 0.656\n",
      "iteration (14800): loss = 0.966, accuracy = 0.617\n",
      "iteration (14850): loss = 1.000, accuracy = 0.617\n",
      "validation for epoch 38\n",
      "-  epoch 38: validation accuracy = 0.104\n",
      "train for epoch 39\n",
      "iteration (14900): loss = 0.992, accuracy = 0.633\n",
      "iteration (14950): loss = 0.994, accuracy = 0.625\n",
      "iteration (15000): loss = 1.175, accuracy = 0.547\n",
      "iteration (15050): loss = 0.988, accuracy = 0.594\n",
      "iteration (15100): loss = 1.180, accuracy = 0.555\n",
      "iteration (15150): loss = 0.962, accuracy = 0.641\n",
      "iteration (15200): loss = 0.975, accuracy = 0.664\n",
      "iteration (15250): loss = 1.141, accuracy = 0.617\n",
      "validation for epoch 39\n",
      "-  epoch 39: validation accuracy = 0.103\n",
      "train for epoch 40\n",
      "iteration (15300): loss = 0.800, accuracy = 0.734\n",
      "iteration (15350): loss = 1.236, accuracy = 0.562\n",
      "iteration (15400): loss = 0.969, accuracy = 0.672\n",
      "iteration (15450): loss = 1.040, accuracy = 0.602\n",
      "iteration (15500): loss = 0.961, accuracy = 0.648\n",
      "iteration (15550): loss = 0.958, accuracy = 0.609\n",
      "iteration (15600): loss = 1.096, accuracy = 0.617\n",
      "iteration (15650): loss = 0.937, accuracy = 0.695\n",
      "validation for epoch 40\n",
      "-  epoch 40: validation accuracy = 0.097\n",
      "train for epoch 41\n",
      "iteration (15700): loss = 1.055, accuracy = 0.719\n",
      "iteration (15750): loss = 0.897, accuracy = 0.672\n",
      "iteration (15800): loss = 1.167, accuracy = 0.578\n",
      "iteration (15850): loss = 0.923, accuracy = 0.719\n",
      "iteration (15900): loss = 1.030, accuracy = 0.617\n",
      "iteration (15950): loss = 1.145, accuracy = 0.562\n",
      "iteration (16000): loss = 1.019, accuracy = 0.633\n",
      "validation for epoch 41\n",
      "-  epoch 41: validation accuracy = 0.090\n",
      "train for epoch 42\n",
      "iteration (16050): loss = 1.079, accuracy = 0.625\n",
      "iteration (16100): loss = 0.898, accuracy = 0.633\n",
      "iteration (16150): loss = 1.016, accuracy = 0.617\n",
      "iteration (16200): loss = 1.010, accuracy = 0.633\n",
      "iteration (16250): loss = 1.014, accuracy = 0.664\n",
      "iteration (16300): loss = 0.955, accuracy = 0.625\n",
      "iteration (16350): loss = 1.022, accuracy = 0.602\n",
      "iteration (16400): loss = 0.925, accuracy = 0.641\n",
      "validation for epoch 42\n",
      "-  epoch 42: validation accuracy = 0.099\n",
      "train for epoch 43\n",
      "iteration (16450): loss = 1.065, accuracy = 0.602\n",
      "iteration (16500): loss = 0.832, accuracy = 0.727\n",
      "iteration (16550): loss = 1.051, accuracy = 0.656\n",
      "iteration (16600): loss = 1.181, accuracy = 0.586\n",
      "iteration (16650): loss = 0.859, accuracy = 0.672\n",
      "iteration (16700): loss = 0.913, accuracy = 0.664\n",
      "iteration (16750): loss = 1.105, accuracy = 0.602\n",
      "iteration (16800): loss = 0.952, accuracy = 0.609\n",
      "validation for epoch 43\n",
      "-  epoch 43: validation accuracy = 0.093\n",
      "train for epoch 44\n",
      "iteration (16850): loss = 1.021, accuracy = 0.602\n",
      "iteration (16900): loss = 1.170, accuracy = 0.586\n",
      "iteration (16950): loss = 0.917, accuracy = 0.672\n",
      "iteration (17000): loss = 0.851, accuracy = 0.711\n",
      "iteration (17050): loss = 0.901, accuracy = 0.734\n",
      "iteration (17100): loss = 0.986, accuracy = 0.641\n",
      "iteration (17150): loss = 1.255, accuracy = 0.578\n",
      "validation for epoch 44\n",
      "-  epoch 44: validation accuracy = 0.092\n",
      "train for epoch 45\n",
      "iteration (17200): loss = 0.922, accuracy = 0.641\n",
      "iteration (17250): loss = 1.034, accuracy = 0.617\n",
      "iteration (17300): loss = 1.023, accuracy = 0.648\n",
      "iteration (17350): loss = 1.001, accuracy = 0.641\n",
      "iteration (17400): loss = 1.026, accuracy = 0.648\n",
      "iteration (17450): loss = 0.963, accuracy = 0.641\n",
      "iteration (17500): loss = 1.142, accuracy = 0.625\n",
      "iteration (17550): loss = 0.776, accuracy = 0.688\n",
      "validation for epoch 45\n",
      "-  epoch 45: validation accuracy = 0.095\n",
      "train for epoch 46\n",
      "iteration (17600): loss = 0.888, accuracy = 0.672\n",
      "iteration (17650): loss = 0.936, accuracy = 0.680\n",
      "iteration (17700): loss = 0.888, accuracy = 0.680\n",
      "iteration (17750): loss = 0.928, accuracy = 0.656\n",
      "iteration (17800): loss = 0.862, accuracy = 0.695\n",
      "iteration (17850): loss = 0.847, accuracy = 0.680\n",
      "iteration (17900): loss = 1.101, accuracy = 0.625\n",
      "iteration (17950): loss = 0.861, accuracy = 0.703\n",
      "validation for epoch 46\n",
      "-  epoch 46: validation accuracy = 0.095\n",
      "train for epoch 47\n",
      "iteration (18000): loss = 0.749, accuracy = 0.742\n",
      "iteration (18050): loss = 0.925, accuracy = 0.656\n",
      "iteration (18100): loss = 0.986, accuracy = 0.641\n",
      "iteration (18150): loss = 1.023, accuracy = 0.656\n",
      "iteration (18200): loss = 0.920, accuracy = 0.680\n",
      "iteration (18250): loss = 0.967, accuracy = 0.617\n",
      "iteration (18300): loss = 0.942, accuracy = 0.664\n",
      "validation for epoch 47\n",
      "-  epoch 47: validation accuracy = 0.100\n",
      "train for epoch 48\n",
      "iteration (18350): loss = 0.836, accuracy = 0.711\n",
      "iteration (18400): loss = 0.945, accuracy = 0.680\n",
      "iteration (18450): loss = 0.954, accuracy = 0.711\n",
      "iteration (18500): loss = 0.859, accuracy = 0.727\n",
      "iteration (18550): loss = 0.975, accuracy = 0.680\n",
      "iteration (18600): loss = 1.028, accuracy = 0.602\n",
      "iteration (18650): loss = 1.027, accuracy = 0.633\n",
      "iteration (18700): loss = 0.802, accuracy = 0.711\n",
      "validation for epoch 48\n",
      "-  epoch 48: validation accuracy = 0.105\n",
      "train for epoch 49\n",
      "iteration (18750): loss = 0.890, accuracy = 0.688\n",
      "iteration (18800): loss = 0.955, accuracy = 0.625\n",
      "iteration (18850): loss = 0.791, accuracy = 0.672\n",
      "iteration (18900): loss = 0.719, accuracy = 0.766\n",
      "iteration (18950): loss = 0.913, accuracy = 0.688\n",
      "iteration (19000): loss = 0.930, accuracy = 0.609\n",
      "iteration (19050): loss = 0.950, accuracy = 0.641\n",
      "validation for epoch 49\n",
      "-  epoch 49: validation accuracy = 0.093\n",
      "train for epoch 50\n",
      "iteration (19100): loss = 0.888, accuracy = 0.680\n",
      "iteration (19150): loss = 0.776, accuracy = 0.680\n",
      "iteration (19200): loss = 0.944, accuracy = 0.648\n",
      "iteration (19250): loss = 0.800, accuracy = 0.711\n",
      "iteration (19300): loss = 1.016, accuracy = 0.672\n",
      "iteration (19350): loss = 0.943, accuracy = 0.633\n",
      "iteration (19400): loss = 0.833, accuracy = 0.688\n",
      "iteration (19450): loss = 0.987, accuracy = 0.602\n",
      "validation for epoch 50\n",
      "-  epoch 50: validation accuracy = 0.097\n",
      "train for epoch 51\n",
      "iteration (19500): loss = 0.947, accuracy = 0.703\n",
      "iteration (19550): loss = 1.017, accuracy = 0.602\n",
      "iteration (19600): loss = 0.970, accuracy = 0.602\n",
      "iteration (19650): loss = 0.817, accuracy = 0.734\n",
      "iteration (19700): loss = 0.926, accuracy = 0.656\n",
      "iteration (19750): loss = 0.998, accuracy = 0.625\n",
      "iteration (19800): loss = 0.974, accuracy = 0.656\n",
      "iteration (19850): loss = 0.740, accuracy = 0.750\n",
      "validation for epoch 51\n",
      "-  epoch 51: validation accuracy = 0.100\n",
      "train for epoch 52\n",
      "iteration (19900): loss = 0.812, accuracy = 0.672\n",
      "iteration (19950): loss = 0.956, accuracy = 0.656\n",
      "iteration (20000): loss = 0.917, accuracy = 0.703\n",
      "iteration (20050): loss = 0.906, accuracy = 0.688\n",
      "iteration (20100): loss = 0.729, accuracy = 0.758\n",
      "iteration (20150): loss = 0.922, accuracy = 0.656\n",
      "iteration (20200): loss = 0.780, accuracy = 0.695\n",
      "validation for epoch 52\n",
      "-  epoch 52: validation accuracy = 0.093\n",
      "train for epoch 53\n",
      "iteration (20250): loss = 0.976, accuracy = 0.648\n",
      "iteration (20300): loss = 0.804, accuracy = 0.727\n",
      "iteration (20350): loss = 0.972, accuracy = 0.680\n",
      "iteration (20400): loss = 1.084, accuracy = 0.586\n",
      "iteration (20450): loss = 0.811, accuracy = 0.734\n",
      "iteration (20500): loss = 0.926, accuracy = 0.695\n",
      "iteration (20550): loss = 0.874, accuracy = 0.648\n",
      "iteration (20600): loss = 0.821, accuracy = 0.719\n",
      "validation for epoch 53\n",
      "-  epoch 53: validation accuracy = 0.104\n",
      "train for epoch 54\n",
      "iteration (20650): loss = 0.749, accuracy = 0.695\n",
      "iteration (20700): loss = 0.944, accuracy = 0.680\n",
      "iteration (20750): loss = 0.854, accuracy = 0.711\n",
      "iteration (20800): loss = 0.999, accuracy = 0.617\n",
      "iteration (20850): loss = 0.883, accuracy = 0.695\n",
      "iteration (20900): loss = 0.893, accuracy = 0.664\n",
      "iteration (20950): loss = 1.010, accuracy = 0.688\n",
      "iteration (21000): loss = 0.922, accuracy = 0.688\n",
      "validation for epoch 54\n",
      "-  epoch 54: validation accuracy = 0.104\n",
      "train for epoch 55\n",
      "iteration (21050): loss = 0.913, accuracy = 0.695\n",
      "iteration (21100): loss = 0.673, accuracy = 0.734\n",
      "iteration (21150): loss = 0.922, accuracy = 0.656\n",
      "iteration (21200): loss = 0.834, accuracy = 0.680\n",
      "iteration (21250): loss = 0.963, accuracy = 0.664\n",
      "iteration (21300): loss = 0.844, accuracy = 0.711\n",
      "iteration (21350): loss = 0.827, accuracy = 0.672\n",
      "validation for epoch 55\n",
      "-  epoch 55: validation accuracy = 0.094\n",
      "train for epoch 56\n",
      "iteration (21400): loss = 0.826, accuracy = 0.648\n",
      "iteration (21450): loss = 0.944, accuracy = 0.672\n",
      "iteration (21500): loss = 0.765, accuracy = 0.742\n",
      "iteration (21550): loss = 1.017, accuracy = 0.617\n",
      "iteration (21600): loss = 0.949, accuracy = 0.711\n",
      "iteration (21650): loss = 0.985, accuracy = 0.648\n",
      "iteration (21700): loss = 0.810, accuracy = 0.719\n",
      "iteration (21750): loss = 0.864, accuracy = 0.680\n",
      "validation for epoch 56\n",
      "-  epoch 56: validation accuracy = 0.099\n",
      "train for epoch 57\n",
      "iteration (21800): loss = 1.184, accuracy = 0.570\n",
      "iteration (21850): loss = 0.982, accuracy = 0.672\n",
      "iteration (21900): loss = 0.856, accuracy = 0.672\n",
      "iteration (21950): loss = 0.892, accuracy = 0.664\n",
      "iteration (22000): loss = 0.778, accuracy = 0.711\n",
      "iteration (22050): loss = 0.894, accuracy = 0.688\n",
      "iteration (22100): loss = 0.696, accuracy = 0.727\n",
      "iteration (22150): loss = 0.836, accuracy = 0.656\n",
      "validation for epoch 57\n",
      "-  epoch 57: validation accuracy = 0.098\n",
      "train for epoch 58\n",
      "iteration (22200): loss = 0.936, accuracy = 0.625\n",
      "iteration (22250): loss = 0.829, accuracy = 0.711\n",
      "iteration (22300): loss = 1.006, accuracy = 0.664\n",
      "iteration (22350): loss = 0.949, accuracy = 0.648\n",
      "iteration (22400): loss = 0.838, accuracy = 0.672\n",
      "iteration (22450): loss = 1.008, accuracy = 0.633\n",
      "iteration (22500): loss = 0.941, accuracy = 0.680\n",
      "validation for epoch 58\n",
      "-  epoch 58: validation accuracy = 0.092\n",
      "train for epoch 59\n",
      "iteration (22550): loss = 0.961, accuracy = 0.672\n",
      "iteration (22600): loss = 0.924, accuracy = 0.656\n",
      "iteration (22650): loss = 0.854, accuracy = 0.641\n",
      "iteration (22700): loss = 0.787, accuracy = 0.719\n",
      "iteration (22750): loss = 0.840, accuracy = 0.719\n",
      "iteration (22800): loss = 0.975, accuracy = 0.641\n",
      "iteration (22850): loss = 0.940, accuracy = 0.688\n",
      "iteration (22900): loss = 0.908, accuracy = 0.711\n",
      "validation for epoch 59\n",
      "-  epoch 59: validation accuracy = 0.109\n",
      "train for epoch 60\n",
      "iteration (22950): loss = 0.832, accuracy = 0.719\n",
      "iteration (23000): loss = 0.840, accuracy = 0.703\n",
      "iteration (23050): loss = 0.737, accuracy = 0.695\n",
      "iteration (23100): loss = 0.829, accuracy = 0.719\n",
      "iteration (23150): loss = 0.845, accuracy = 0.727\n",
      "iteration (23200): loss = 0.938, accuracy = 0.664\n",
      "iteration (23250): loss = 0.679, accuracy = 0.758\n",
      "iteration (23300): loss = 0.837, accuracy = 0.727\n",
      "validation for epoch 60\n",
      "-  epoch 60: validation accuracy = 0.100\n",
      "train for epoch 61\n",
      "iteration (23350): loss = 0.968, accuracy = 0.664\n",
      "iteration (23400): loss = 0.976, accuracy = 0.648\n",
      "iteration (23450): loss = 0.839, accuracy = 0.680\n",
      "iteration (23500): loss = 0.815, accuracy = 0.719\n",
      "iteration (23550): loss = 0.906, accuracy = 0.656\n",
      "iteration (23600): loss = 0.805, accuracy = 0.719\n",
      "iteration (23650): loss = 0.939, accuracy = 0.695\n",
      "validation for epoch 61\n",
      "-  epoch 61: validation accuracy = 0.106\n",
      "train for epoch 62\n",
      "iteration (23700): loss = 0.691, accuracy = 0.750\n",
      "iteration (23750): loss = 0.906, accuracy = 0.648\n",
      "iteration (23800): loss = 0.975, accuracy = 0.633\n",
      "iteration (23850): loss = 0.924, accuracy = 0.672\n",
      "iteration (23900): loss = 0.789, accuracy = 0.727\n",
      "iteration (23950): loss = 0.774, accuracy = 0.727\n",
      "iteration (24000): loss = 0.807, accuracy = 0.742\n",
      "iteration (24050): loss = 0.755, accuracy = 0.703\n",
      "validation for epoch 62\n",
      "-  epoch 62: validation accuracy = 0.098\n",
      "train for epoch 63\n",
      "iteration (24100): loss = 0.797, accuracy = 0.727\n",
      "iteration (24150): loss = 0.757, accuracy = 0.727\n",
      "iteration (24200): loss = 0.852, accuracy = 0.688\n",
      "iteration (24250): loss = 0.813, accuracy = 0.672\n",
      "iteration (24300): loss = 0.753, accuracy = 0.711\n",
      "iteration (24350): loss = 0.740, accuracy = 0.703\n",
      "iteration (24400): loss = 0.816, accuracy = 0.703\n",
      "validation for epoch 63\n",
      "-  epoch 63: validation accuracy = 0.099\n",
      "train for epoch 64\n",
      "iteration (24450): loss = 0.933, accuracy = 0.672\n",
      "iteration (24500): loss = 0.784, accuracy = 0.758\n",
      "iteration (24550): loss = 0.865, accuracy = 0.680\n",
      "iteration (24600): loss = 0.881, accuracy = 0.672\n",
      "iteration (24650): loss = 1.044, accuracy = 0.648\n",
      "iteration (24700): loss = 0.819, accuracy = 0.734\n",
      "iteration (24750): loss = 0.846, accuracy = 0.688\n",
      "iteration (24800): loss = 0.752, accuracy = 0.703\n",
      "validation for epoch 64\n",
      "-  epoch 64: validation accuracy = 0.112\n",
      "train for epoch 65\n",
      "iteration (24850): loss = 0.784, accuracy = 0.695\n",
      "iteration (24900): loss = 1.178, accuracy = 0.562\n",
      "iteration (24950): loss = 0.741, accuracy = 0.750\n",
      "iteration (25000): loss = 0.899, accuracy = 0.688\n",
      "iteration (25050): loss = 0.740, accuracy = 0.711\n",
      "iteration (25100): loss = 0.907, accuracy = 0.656\n",
      "iteration (25150): loss = 0.876, accuracy = 0.711\n",
      "iteration (25200): loss = 0.887, accuracy = 0.695\n",
      "validation for epoch 65\n",
      "-  epoch 65: validation accuracy = 0.110\n",
      "train for epoch 66\n",
      "iteration (25250): loss = 1.010, accuracy = 0.672\n",
      "iteration (25300): loss = 0.910, accuracy = 0.664\n",
      "iteration (25350): loss = 1.102, accuracy = 0.641\n",
      "iteration (25400): loss = 0.829, accuracy = 0.742\n",
      "iteration (25450): loss = 0.864, accuracy = 0.711\n",
      "iteration (25500): loss = 0.962, accuracy = 0.664\n",
      "iteration (25550): loss = 0.869, accuracy = 0.656\n",
      "validation for epoch 66\n",
      "-  epoch 66: validation accuracy = 0.107\n",
      "train for epoch 67\n",
      "iteration (25600): loss = 0.812, accuracy = 0.727\n",
      "iteration (25650): loss = 0.748, accuracy = 0.719\n",
      "iteration (25700): loss = 0.881, accuracy = 0.648\n",
      "iteration (25750): loss = 0.785, accuracy = 0.688\n",
      "iteration (25800): loss = 0.837, accuracy = 0.727\n",
      "iteration (25850): loss = 0.820, accuracy = 0.734\n",
      "iteration (25900): loss = 0.905, accuracy = 0.688\n",
      "iteration (25950): loss = 0.878, accuracy = 0.633\n",
      "validation for epoch 67\n",
      "-  epoch 67: validation accuracy = 0.107\n",
      "train for epoch 68\n",
      "iteration (26000): loss = 0.782, accuracy = 0.719\n",
      "iteration (26050): loss = 0.719, accuracy = 0.711\n",
      "iteration (26100): loss = 0.840, accuracy = 0.703\n",
      "iteration (26150): loss = 0.758, accuracy = 0.742\n",
      "iteration (26200): loss = 0.766, accuracy = 0.688\n",
      "iteration (26250): loss = 0.756, accuracy = 0.719\n",
      "iteration (26300): loss = 0.996, accuracy = 0.641\n",
      "iteration (26350): loss = 0.926, accuracy = 0.703\n",
      "validation for epoch 68\n",
      "-  epoch 68: validation accuracy = 0.112\n",
      "train for epoch 69\n",
      "iteration (26400): loss = 0.999, accuracy = 0.641\n",
      "iteration (26450): loss = 0.925, accuracy = 0.680\n",
      "iteration (26500): loss = 0.844, accuracy = 0.711\n",
      "iteration (26550): loss = 0.731, accuracy = 0.727\n",
      "iteration (26600): loss = 0.859, accuracy = 0.711\n",
      "iteration (26650): loss = 0.854, accuracy = 0.742\n",
      "iteration (26700): loss = 0.872, accuracy = 0.688\n",
      "validation for epoch 69\n",
      "-  epoch 69: validation accuracy = 0.114\n",
      "train for epoch 70\n",
      "iteration (26750): loss = 0.782, accuracy = 0.758\n",
      "iteration (26800): loss = 0.856, accuracy = 0.711\n",
      "iteration (26850): loss = 0.824, accuracy = 0.750\n",
      "iteration (26900): loss = 0.885, accuracy = 0.703\n",
      "iteration (26950): loss = 0.997, accuracy = 0.680\n",
      "iteration (27000): loss = 0.881, accuracy = 0.641\n",
      "iteration (27050): loss = 0.926, accuracy = 0.688\n",
      "iteration (27100): loss = 0.632, accuracy = 0.750\n",
      "validation for epoch 70\n",
      "-  epoch 70: validation accuracy = 0.112\n",
      "train for epoch 71\n",
      "iteration (27150): loss = 0.670, accuracy = 0.766\n",
      "iteration (27200): loss = 0.754, accuracy = 0.672\n",
      "iteration (27250): loss = 0.736, accuracy = 0.719\n",
      "iteration (27300): loss = 0.897, accuracy = 0.703\n",
      "iteration (27350): loss = 0.732, accuracy = 0.719\n",
      "iteration (27400): loss = 0.767, accuracy = 0.742\n",
      "iteration (27450): loss = 0.920, accuracy = 0.664\n",
      "iteration (27500): loss = 0.756, accuracy = 0.727\n",
      "validation for epoch 71\n",
      "-  epoch 71: validation accuracy = 0.113\n",
      "train for epoch 72\n",
      "iteration (27550): loss = 0.698, accuracy = 0.727\n",
      "iteration (27600): loss = 0.997, accuracy = 0.656\n",
      "iteration (27650): loss = 0.884, accuracy = 0.703\n",
      "iteration (27700): loss = 0.847, accuracy = 0.742\n",
      "iteration (27750): loss = 0.852, accuracy = 0.711\n",
      "iteration (27800): loss = 0.912, accuracy = 0.695\n",
      "iteration (27850): loss = 0.895, accuracy = 0.711\n",
      "validation for epoch 72\n",
      "-  epoch 72: validation accuracy = 0.106\n",
      "train for epoch 73\n",
      "iteration (27900): loss = 0.792, accuracy = 0.750\n",
      "iteration (27950): loss = 0.947, accuracy = 0.664\n",
      "iteration (28000): loss = 0.762, accuracy = 0.734\n",
      "iteration (28050): loss = 0.846, accuracy = 0.711\n",
      "iteration (28100): loss = 0.913, accuracy = 0.719\n",
      "iteration (28150): loss = 0.831, accuracy = 0.688\n",
      "iteration (28200): loss = 0.860, accuracy = 0.719\n",
      "iteration (28250): loss = 0.940, accuracy = 0.688\n",
      "validation for epoch 73\n",
      "-  epoch 73: validation accuracy = 0.099\n",
      "train for epoch 74\n",
      "iteration (28300): loss = 0.841, accuracy = 0.680\n",
      "iteration (28350): loss = 0.749, accuracy = 0.688\n",
      "iteration (28400): loss = 0.720, accuracy = 0.688\n",
      "iteration (28450): loss = 0.635, accuracy = 0.828\n",
      "iteration (28500): loss = 0.701, accuracy = 0.742\n",
      "iteration (28550): loss = 0.849, accuracy = 0.703\n",
      "iteration (28600): loss = 0.878, accuracy = 0.695\n",
      "validation for epoch 74\n",
      "-  epoch 74: validation accuracy = 0.112\n",
      "train for epoch 75\n",
      "iteration (28650): loss = 0.784, accuracy = 0.688\n",
      "iteration (28700): loss = 0.661, accuracy = 0.781\n",
      "iteration (28750): loss = 0.815, accuracy = 0.703\n",
      "iteration (28800): loss = 0.725, accuracy = 0.750\n",
      "iteration (28850): loss = 0.978, accuracy = 0.688\n",
      "iteration (28900): loss = 0.829, accuracy = 0.703\n",
      "iteration (28950): loss = 0.697, accuracy = 0.742\n",
      "iteration (29000): loss = 0.899, accuracy = 0.703\n",
      "validation for epoch 75\n",
      "-  epoch 75: validation accuracy = 0.109\n",
      "train for epoch 76\n",
      "iteration (29050): loss = 0.832, accuracy = 0.734\n",
      "iteration (29100): loss = 0.845, accuracy = 0.672\n",
      "iteration (29150): loss = 0.870, accuracy = 0.680\n",
      "iteration (29200): loss = 0.714, accuracy = 0.703\n",
      "iteration (29250): loss = 0.824, accuracy = 0.688\n",
      "iteration (29300): loss = 0.869, accuracy = 0.688\n",
      "iteration (29350): loss = 0.764, accuracy = 0.711\n",
      "iteration (29400): loss = 0.633, accuracy = 0.766\n",
      "validation for epoch 76\n",
      "-  epoch 76: validation accuracy = 0.109\n",
      "train for epoch 77\n",
      "iteration (29450): loss = 0.681, accuracy = 0.766\n",
      "iteration (29500): loss = 0.852, accuracy = 0.688\n",
      "iteration (29550): loss = 0.733, accuracy = 0.773\n",
      "iteration (29600): loss = 0.712, accuracy = 0.742\n",
      "iteration (29650): loss = 0.579, accuracy = 0.805\n",
      "iteration (29700): loss = 0.772, accuracy = 0.711\n",
      "iteration (29750): loss = 0.680, accuracy = 0.742\n",
      "validation for epoch 77\n",
      "-  epoch 77: validation accuracy = 0.110\n",
      "train for epoch 78\n",
      "iteration (29800): loss = 0.938, accuracy = 0.648\n",
      "iteration (29850): loss = 0.780, accuracy = 0.750\n",
      "iteration (29900): loss = 0.925, accuracy = 0.734\n",
      "iteration (29950): loss = 0.919, accuracy = 0.641\n",
      "iteration (30000): loss = 0.718, accuracy = 0.711\n",
      "iteration (30050): loss = 0.872, accuracy = 0.711\n",
      "iteration (30100): loss = 0.730, accuracy = 0.742\n",
      "iteration (30150): loss = 0.580, accuracy = 0.797\n",
      "validation for epoch 78\n",
      "-  epoch 78: validation accuracy = 0.109\n",
      "train for epoch 79\n",
      "iteration (30200): loss = 0.692, accuracy = 0.750\n",
      "iteration (30250): loss = 0.751, accuracy = 0.727\n",
      "iteration (30300): loss = 1.014, accuracy = 0.672\n",
      "iteration (30350): loss = 0.837, accuracy = 0.672\n",
      "iteration (30400): loss = 0.717, accuracy = 0.766\n",
      "iteration (30450): loss = 0.778, accuracy = 0.688\n",
      "iteration (30500): loss = 0.904, accuracy = 0.711\n",
      "iteration (30550): loss = 0.938, accuracy = 0.586\n",
      "validation for epoch 79\n",
      "-  epoch 79: validation accuracy = 0.108\n",
      "train for epoch 80\n",
      "iteration (30600): loss = 0.856, accuracy = 0.656\n",
      "iteration (30650): loss = 0.588, accuracy = 0.812\n",
      "iteration (30700): loss = 0.976, accuracy = 0.617\n",
      "iteration (30750): loss = 0.824, accuracy = 0.688\n",
      "iteration (30800): loss = 0.884, accuracy = 0.656\n",
      "iteration (30850): loss = 0.844, accuracy = 0.719\n",
      "iteration (30900): loss = 0.862, accuracy = 0.727\n",
      "validation for epoch 80\n",
      "-  epoch 80: validation accuracy = 0.108\n",
      "train for epoch 81\n",
      "iteration (30950): loss = 0.642, accuracy = 0.742\n",
      "iteration (31000): loss = 0.774, accuracy = 0.719\n",
      "iteration (31050): loss = 0.827, accuracy = 0.719\n",
      "iteration (31100): loss = 1.028, accuracy = 0.641\n",
      "iteration (31150): loss = 0.804, accuracy = 0.719\n",
      "iteration (31200): loss = 0.856, accuracy = 0.648\n",
      "iteration (31250): loss = 0.773, accuracy = 0.758\n",
      "iteration (31300): loss = 0.785, accuracy = 0.688\n",
      "validation for epoch 81\n",
      "-  epoch 81: validation accuracy = 0.107\n",
      "train for epoch 82\n",
      "iteration (31350): loss = 0.992, accuracy = 0.672\n",
      "iteration (31400): loss = 0.875, accuracy = 0.680\n",
      "iteration (31450): loss = 0.854, accuracy = 0.703\n",
      "iteration (31500): loss = 0.775, accuracy = 0.750\n",
      "iteration (31550): loss = 0.617, accuracy = 0.789\n",
      "iteration (31600): loss = 0.749, accuracy = 0.734\n",
      "iteration (31650): loss = 0.715, accuracy = 0.766\n",
      "iteration (31700): loss = 0.752, accuracy = 0.742\n",
      "validation for epoch 82\n",
      "-  epoch 82: validation accuracy = 0.109\n",
      "train for epoch 83\n",
      "iteration (31750): loss = 0.834, accuracy = 0.680\n",
      "iteration (31800): loss = 0.854, accuracy = 0.680\n",
      "iteration (31850): loss = 0.975, accuracy = 0.656\n",
      "iteration (31900): loss = 0.819, accuracy = 0.703\n",
      "iteration (31950): loss = 0.681, accuracy = 0.758\n",
      "iteration (32000): loss = 0.882, accuracy = 0.688\n",
      "iteration (32050): loss = 0.897, accuracy = 0.711\n",
      "validation for epoch 83\n",
      "-  epoch 83: validation accuracy = 0.109\n",
      "train for epoch 84\n",
      "iteration (32100): loss = 0.871, accuracy = 0.695\n",
      "iteration (32150): loss = 0.949, accuracy = 0.656\n",
      "iteration (32200): loss = 0.780, accuracy = 0.719\n",
      "iteration (32250): loss = 0.856, accuracy = 0.711\n",
      "iteration (32300): loss = 0.741, accuracy = 0.734\n",
      "iteration (32350): loss = 0.910, accuracy = 0.664\n",
      "iteration (32400): loss = 0.837, accuracy = 0.711\n",
      "iteration (32450): loss = 0.812, accuracy = 0.711\n",
      "validation for epoch 84\n",
      "-  epoch 84: validation accuracy = 0.116\n",
      "train for epoch 85\n",
      "iteration (32500): loss = 0.851, accuracy = 0.727\n",
      "iteration (32550): loss = 0.858, accuracy = 0.766\n",
      "iteration (32600): loss = 0.756, accuracy = 0.727\n",
      "iteration (32650): loss = 0.759, accuracy = 0.719\n",
      "iteration (32700): loss = 0.707, accuracy = 0.805\n",
      "iteration (32750): loss = 0.998, accuracy = 0.656\n",
      "iteration (32800): loss = 0.709, accuracy = 0.773\n",
      "iteration (32850): loss = 0.845, accuracy = 0.656\n",
      "validation for epoch 85\n",
      "-  epoch 85: validation accuracy = 0.109\n",
      "train for epoch 86\n",
      "iteration (32900): loss = 0.790, accuracy = 0.742\n",
      "iteration (32950): loss = 0.821, accuracy = 0.688\n",
      "iteration (33000): loss = 0.806, accuracy = 0.672\n",
      "iteration (33050): loss = 0.899, accuracy = 0.641\n",
      "iteration (33100): loss = 0.909, accuracy = 0.664\n",
      "iteration (33150): loss = 0.851, accuracy = 0.742\n",
      "iteration (33200): loss = 0.847, accuracy = 0.727\n",
      "validation for epoch 86\n",
      "-  epoch 86: validation accuracy = 0.110\n",
      "train for epoch 87\n",
      "iteration (33250): loss = 0.625, accuracy = 0.766\n",
      "iteration (33300): loss = 0.799, accuracy = 0.680\n",
      "iteration (33350): loss = 0.956, accuracy = 0.656\n",
      "iteration (33400): loss = 0.955, accuracy = 0.695\n",
      "iteration (33450): loss = 0.738, accuracy = 0.773\n",
      "iteration (33500): loss = 0.653, accuracy = 0.766\n",
      "iteration (33550): loss = 0.685, accuracy = 0.727\n",
      "iteration (33600): loss = 0.725, accuracy = 0.734\n",
      "validation for epoch 87\n",
      "-  epoch 87: validation accuracy = 0.105\n",
      "train for epoch 88\n",
      "iteration (33650): loss = 0.686, accuracy = 0.797\n",
      "iteration (33700): loss = 0.767, accuracy = 0.742\n",
      "iteration (33750): loss = 0.704, accuracy = 0.695\n",
      "iteration (33800): loss = 0.702, accuracy = 0.773\n",
      "iteration (33850): loss = 0.681, accuracy = 0.734\n",
      "iteration (33900): loss = 0.807, accuracy = 0.680\n",
      "iteration (33950): loss = 0.764, accuracy = 0.734\n",
      "validation for epoch 88\n",
      "-  epoch 88: validation accuracy = 0.110\n",
      "train for epoch 89\n",
      "iteration (34000): loss = 0.813, accuracy = 0.711\n",
      "iteration (34050): loss = 0.697, accuracy = 0.773\n",
      "iteration (34100): loss = 0.734, accuracy = 0.719\n",
      "iteration (34150): loss = 0.878, accuracy = 0.648\n",
      "iteration (34200): loss = 0.901, accuracy = 0.656\n",
      "iteration (34250): loss = 0.644, accuracy = 0.766\n",
      "iteration (34300): loss = 0.782, accuracy = 0.711\n",
      "iteration (34350): loss = 0.857, accuracy = 0.727\n",
      "validation for epoch 89\n",
      "-  epoch 89: validation accuracy = 0.112\n",
      "train for epoch 90\n",
      "iteration (34400): loss = 0.712, accuracy = 0.766\n",
      "iteration (34450): loss = 0.965, accuracy = 0.680\n",
      "iteration (34500): loss = 0.674, accuracy = 0.797\n",
      "iteration (34550): loss = 0.706, accuracy = 0.758\n",
      "iteration (34600): loss = 0.692, accuracy = 0.758\n",
      "iteration (34650): loss = 0.737, accuracy = 0.719\n",
      "iteration (34700): loss = 0.936, accuracy = 0.695\n",
      "iteration (34750): loss = 0.829, accuracy = 0.664\n",
      "validation for epoch 90\n",
      "-  epoch 90: validation accuracy = 0.106\n",
      "train for epoch 91\n",
      "iteration (34800): loss = 0.738, accuracy = 0.742\n",
      "iteration (34850): loss = 0.709, accuracy = 0.719\n",
      "iteration (34900): loss = 0.889, accuracy = 0.680\n",
      "iteration (34950): loss = 0.708, accuracy = 0.750\n",
      "iteration (35000): loss = 0.739, accuracy = 0.750\n",
      "iteration (35050): loss = 0.854, accuracy = 0.719\n",
      "iteration (35100): loss = 0.677, accuracy = 0.742\n",
      "validation for epoch 91\n",
      "-  epoch 91: validation accuracy = 0.110\n",
      "train for epoch 92\n",
      "iteration (35150): loss = 0.926, accuracy = 0.656\n",
      "iteration (35200): loss = 0.678, accuracy = 0.711\n",
      "iteration (35250): loss = 0.807, accuracy = 0.672\n",
      "iteration (35300): loss = 0.774, accuracy = 0.750\n",
      "iteration (35350): loss = 0.826, accuracy = 0.711\n",
      "iteration (35400): loss = 0.907, accuracy = 0.680\n",
      "iteration (35450): loss = 0.810, accuracy = 0.719\n",
      "iteration (35500): loss = 0.853, accuracy = 0.648\n",
      "validation for epoch 92\n",
      "-  epoch 92: validation accuracy = 0.115\n",
      "train for epoch 93\n",
      "iteration (35550): loss = 0.773, accuracy = 0.703\n",
      "iteration (35600): loss = 0.803, accuracy = 0.695\n",
      "iteration (35650): loss = 0.755, accuracy = 0.758\n",
      "iteration (35700): loss = 0.943, accuracy = 0.680\n",
      "iteration (35750): loss = 0.754, accuracy = 0.703\n",
      "iteration (35800): loss = 0.760, accuracy = 0.734\n",
      "iteration (35850): loss = 0.978, accuracy = 0.625\n",
      "iteration (35900): loss = 0.760, accuracy = 0.734\n",
      "validation for epoch 93\n",
      "-  epoch 93: validation accuracy = 0.109\n",
      "train for epoch 94\n",
      "iteration (35950): loss = 0.832, accuracy = 0.734\n",
      "iteration (36000): loss = 0.828, accuracy = 0.719\n",
      "iteration (36050): loss = 0.866, accuracy = 0.633\n",
      "iteration (36100): loss = 0.645, accuracy = 0.742\n",
      "iteration (36150): loss = 0.713, accuracy = 0.773\n",
      "iteration (36200): loss = 0.841, accuracy = 0.727\n",
      "iteration (36250): loss = 0.996, accuracy = 0.656\n",
      "validation for epoch 94\n",
      "-  epoch 94: validation accuracy = 0.108\n",
      "train for epoch 95\n",
      "iteration (36300): loss = 0.734, accuracy = 0.719\n",
      "iteration (36350): loss = 0.851, accuracy = 0.711\n",
      "iteration (36400): loss = 0.915, accuracy = 0.695\n",
      "iteration (36450): loss = 0.896, accuracy = 0.727\n",
      "iteration (36500): loss = 0.842, accuracy = 0.680\n",
      "iteration (36550): loss = 0.705, accuracy = 0.719\n",
      "iteration (36600): loss = 0.774, accuracy = 0.734\n",
      "iteration (36650): loss = 0.735, accuracy = 0.734\n",
      "validation for epoch 95\n",
      "-  epoch 95: validation accuracy = 0.110\n",
      "train for epoch 96\n",
      "iteration (36700): loss = 0.620, accuracy = 0.789\n",
      "iteration (36750): loss = 0.678, accuracy = 0.750\n",
      "iteration (36800): loss = 0.726, accuracy = 0.750\n",
      "iteration (36850): loss = 0.800, accuracy = 0.727\n",
      "iteration (36900): loss = 0.615, accuracy = 0.766\n",
      "iteration (36950): loss = 0.716, accuracy = 0.773\n",
      "iteration (37000): loss = 0.763, accuracy = 0.719\n",
      "iteration (37050): loss = 0.720, accuracy = 0.781\n",
      "validation for epoch 96\n",
      "-  epoch 96: validation accuracy = 0.110\n",
      "train for epoch 97\n",
      "iteration (37100): loss = 0.737, accuracy = 0.727\n",
      "iteration (37150): loss = 0.769, accuracy = 0.742\n",
      "iteration (37200): loss = 0.827, accuracy = 0.680\n",
      "iteration (37250): loss = 0.677, accuracy = 0.750\n",
      "iteration (37300): loss = 0.745, accuracy = 0.703\n",
      "iteration (37350): loss = 0.828, accuracy = 0.695\n",
      "iteration (37400): loss = 0.816, accuracy = 0.703\n",
      "validation for epoch 97\n",
      "-  epoch 97: validation accuracy = 0.108\n",
      "train for epoch 98\n",
      "iteration (37450): loss = 0.778, accuracy = 0.742\n",
      "iteration (37500): loss = 0.775, accuracy = 0.719\n",
      "iteration (37550): loss = 0.743, accuracy = 0.719\n",
      "iteration (37600): loss = 0.681, accuracy = 0.773\n",
      "iteration (37650): loss = 0.768, accuracy = 0.727\n",
      "iteration (37700): loss = 0.824, accuracy = 0.742\n",
      "iteration (37750): loss = 0.842, accuracy = 0.719\n",
      "iteration (37800): loss = 0.770, accuracy = 0.703\n",
      "validation for epoch 98\n",
      "-  epoch 98: validation accuracy = 0.110\n",
      "train for epoch 99\n",
      "iteration (37850): loss = 0.872, accuracy = 0.688\n",
      "iteration (37900): loss = 0.706, accuracy = 0.781\n",
      "iteration (37950): loss = 0.635, accuracy = 0.750\n",
      "iteration (38000): loss = 0.537, accuracy = 0.844\n",
      "iteration (38050): loss = 0.672, accuracy = 0.758\n",
      "iteration (38100): loss = 0.844, accuracy = 0.719\n",
      "iteration (38150): loss = 0.679, accuracy = 0.750\n",
      "validation for epoch 99\n",
      "-  epoch 99: validation accuracy = 0.107\n",
      "***** test accuracy: 0.112\n",
      "Model saved in lib/tf_models/problem2/csci-599_mine.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Preprocessing                                                       #\n",
    "#############################################################################\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_ = scaler.fit_transform(X_train.reshape(-1,32*32*3))\n",
    "# X_train_ = X_train.reshape(-1,32,32,3)\n",
    "# X_val_ = scaler.fit_transform(X_val.reshape(-1,32*32*3))\n",
    "# X_val_ = X_val_.reshape(-1,32,32,3)\n",
    "# X_test_ = scaler.fit_transform(X_test.reshape(-1,32*32*3))\n",
    "# X_test_ = X_test_.reshape(-1, 32,32,3)\n",
    "X_train_ = X_train\n",
    "X_val_= X_val\n",
    "X_test_ = X_test\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "\n",
    "model = YourModel()\n",
    "model.train(sess, X_train_, Y_train, X_val_, Y_val)\n",
    "accuracy = model.evaluate(sess, X_test_, Y_test)\n",
    "print('***** test accuracy: %.3f' % accuracy)\n",
    "\n",
    "# Save your model\n",
    "saver = tf.train.Saver()\n",
    "model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")\n",
    "print(\"Model saved in %s\" % model_path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Load your model\n",
    "model = YourModel()\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
