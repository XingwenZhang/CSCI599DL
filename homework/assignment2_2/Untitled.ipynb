{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45090739  0.62025541  0.98416405  0.75267548]\n",
      " [ 0.25201585  0.32005539  0.64450367  0.09761916]\n",
      " [ 0.13408589  0.92824841  0.46274315  0.90570177]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(3, 4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07659958]\n",
      " [ 0.79836877]\n",
      " [ 0.53462164]]\n"
     ]
    }
   ],
   "source": [
    "b = np.random.rand(3,1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10376286,  0.51344995,  0.40365633,  0.82620169],\n",
       "       [ 0.56775866,  0.4386381 ,  0.06429655,  0.14261394],\n",
       "       [ 0.02849669,  0.30436154,  0.29971836,  0.22502206]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22 33 78]\n",
      " [34 40 18]\n",
      " [81 32 87]\n",
      " [25 31 89]]\n",
      "[[3 2 2 2]\n",
      " [2 1 1 3]\n",
      " [3 2 2 3]\n",
      " [3 1 3 1]\n",
      " [1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "w = np.random.randint(low = 1, high = 100,size = (4,3))\n",
    "x = np.random.randint(low = 1, high = 4, size = (5,4))\n",
    "print(w)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = w[x,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[25, 31, 89],\n",
       "        [81, 32, 87],\n",
       "        [81, 32, 87],\n",
       "        [81, 32, 87]],\n",
       "\n",
       "       [[81, 32, 87],\n",
       "        [34, 40, 18],\n",
       "        [34, 40, 18],\n",
       "        [25, 31, 89]],\n",
       "\n",
       "       [[25, 31, 89],\n",
       "        [81, 32, 87],\n",
       "        [81, 32, 87],\n",
       "        [25, 31, 89]],\n",
       "\n",
       "       [[25, 31, 89],\n",
       "        [34, 40, 18],\n",
       "        [25, 31, 89],\n",
       "        [34, 40, 18]],\n",
       "\n",
       "       [[34, 40, 18],\n",
       "        [34, 40, 18],\n",
       "        [34, 40, 18],\n",
       "        [81, 32, 87]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out2 = w[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[25, 31, 89],\n",
       "        [81, 32, 87],\n",
       "        [81, 32, 87],\n",
       "        [81, 32, 87]],\n",
       "\n",
       "       [[81, 32, 87],\n",
       "        [34, 40, 18],\n",
       "        [34, 40, 18],\n",
       "        [25, 31, 89]],\n",
       "\n",
       "       [[25, 31, 89],\n",
       "        [81, 32, 87],\n",
       "        [81, 32, 87],\n",
       "        [25, 31, 89]],\n",
       "\n",
       "       [[25, 31, 89],\n",
       "        [34, 40, 18],\n",
       "        [25, 31, 89],\n",
       "        [34, 40, 18]],\n",
       "\n",
       "       [[34, 40, 18],\n",
       "        [34, 40, 18],\n",
       "        [34, 40, 18],\n",
       "        [81, 32, 87]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.array([[ 0,  1,  2],\n",
    "            [ 3,  4,  5],\n",
    "            [ 6,  7,  8],\n",
    "            [ 9, 10, 11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = np.array([[0, 1],\n",
    "                  [2, 3]], dtype=np.intp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2],\n",
       "        [ 3,  4,  5]],\n",
       "\n",
       "       [[ 6,  7,  8],\n",
       "        [ 9, 10, 11]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[rows].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A= np.random.rand(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d=np.hsplit(A, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.arange(35).reshape(5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "       [ 7,  8,  9, 10, 11, 12, 13],\n",
       "       [14, 15, 16, 17, 18, 19, 20],\n",
       "       [21, 22, 23, 24, 25, 26, 27],\n",
       "       [28, 29, 30, 31, 32, 33, 34]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 10, 13],\n",
       "       [14, 17, 20],\n",
       "       [21, 24, 27],\n",
       "       [28, 31, 34]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1:5:1,::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-aa78870a13c7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-aa78870a13c7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1:5:2\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1:5:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-9a0c050c6fc1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-9a0c050c6fc1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    np.array([1:5:2])\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "np.array([1:5:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.random.rand(1,2)\n",
    "b=np.random.rand(1,2)\n",
    "c=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a == b).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.zeros(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57252755,  0.38328058]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.random.rand(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 5)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.59484746,  0.52258372,  0.61078393,  0.85177248,  0.09182054],\n",
       "        [ 0.01811635,  0.80221279,  0.34864065,  0.78198546,  0.42445534],\n",
       "        [ 0.565409  ,  0.38552552,  0.41899255,  0.07675478,  0.57957213],\n",
       "        [ 0.65947275,  0.41463278,  0.22739912,  0.86771082,  0.5317409 ]],\n",
       "\n",
       "       [[ 0.21264615,  0.33611993,  0.89509987,  0.86328046,  0.5612715 ],\n",
       "        [ 0.46758191,  0.29738196,  0.17317478,  0.03078538,  0.32954787],\n",
       "        [ 0.00324105,  0.29843512,  0.9692797 ,  0.44888609,  0.31543047],\n",
       "        [ 0.38048198,  0.55009171,  0.77284496,  0.33442358,  0.25762655]],\n",
       "\n",
       "       [[ 0.43412264,  0.02827113,  0.72198659,  0.12308095,  0.77038261],\n",
       "        [ 0.32254853,  0.10939598,  0.5760574 ,  0.29347701,  0.59036259],\n",
       "        [ 0.96663996,  0.047971  ,  0.38196214,  0.60198856,  0.3597749 ],\n",
       "        [ 0.01009767,  0.43171328,  0.40483473,  0.64884542,  0.34604241]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01811635,  0.80221279,  0.34864065,  0.78198546,  0.42445534],\n",
       "       [ 0.46758191,  0.29738196,  0.17317478,  0.03078538,  0.32954787],\n",
       "       [ 0.32254853,  0.10939598,  0.5760574 ,  0.29347701,  0.59036259]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_flat = np.random.randint(0,3,size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_flat = np.random.randint(1,5, size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = np.random.rand(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -np.sum(mask_flat * np.log(probs[np.arange(100), label_flat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 4, 2, 2, 2, 2, 3, 1, 4, 4, 4, 2, 4, 4, 2, 1, 4, 3, 4, 2, 2,\n",
       "       4, 4, 4, 3, 4, 4, 2, 4, 1, 3, 1, 3, 4, 4, 1, 1, 1, 2, 4, 1, 4, 4, 2,\n",
       "       3, 1, 3, 3, 2, 1, 4, 4, 3, 1, 4, 1, 1, 1, 1, 1, 4, 3, 3, 4, 4, 2, 1,\n",
       "       4, 4, 3, 3, 2, 4, 2, 2, 4, 1, 2, 3, 1, 1, 3, 2, 4, 2, 1, 2, 1, 4, 1,\n",
       "       2, 3, 1, 2, 1, 3, 1, 3])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41465286,  0.93406175,  0.34750821,  0.1054345 ,  0.00825942,\n",
       "        0.21962305,  0.81139953,  0.01155261,  0.14701674,  0.90087107,\n",
       "        0.29823861,  0.45452563,  0.43364143,  0.77503435,  0.48601755,\n",
       "        0.26764277,  0.63772334,  0.07723547,  0.07539647,  0.89194061,\n",
       "        0.45063933,  0.61410142,  0.87722017,  0.31459981,  0.00152137,\n",
       "        0.11605555,  0.85612286,  0.81493785,  0.14220529,  0.682943  ,\n",
       "        0.58659284,  0.66167247,  0.6995743 ,  0.7460492 ,  0.96998329,\n",
       "        0.83868617,  0.5847659 ,  0.35365198,  0.81206693,  0.26747103,\n",
       "        0.23444493,  0.13182878,  0.72664246,  0.75119562,  0.84780153,\n",
       "        0.23782185,  0.35472198,  0.06169082,  0.50054648,  0.09205211,\n",
       "        0.13490044,  0.38860806,  0.2585897 ,  0.43119099,  0.59964312,\n",
       "        0.41200012,  0.28676601,  0.55110217,  0.5328486 ,  0.1993818 ,\n",
       "        0.72237674,  0.32108867,  0.70765583,  0.30949215,  0.54115011,\n",
       "        0.96052655,  0.87693261,  0.50946491,  0.04772315,  0.42632942,\n",
       "        0.42665365,  0.64818611,  0.80133226,  0.31559252,  0.90334413,\n",
       "        0.98520602,  0.48236585,  0.95871962,  0.72413591,  0.16997001,\n",
       "        0.60506394,  0.13966425,  0.94533495,  0.62412738,  0.33281257,\n",
       "        0.09023948,  0.19302878,  0.63788612,  0.9137912 ,  0.78040979,\n",
       "        0.58966354,  0.82402597,  0.22981051,  0.88907458,  0.12343524,\n",
       "        0.80192068,  0.32792342,  0.99569913,  0.39674341,  0.96292901])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[np.arange(100), label_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22069989,  0.74961172,  0.79530211,  0.78183146,  0.16803495,\n",
       "         0.38167515,  0.8614896 ,  0.87617332,  0.16445302,  0.41314812],\n",
       "       [ 0.47200263,  0.94029858,  0.9596859 ,  0.53286129,  0.34550472,\n",
       "         0.70770054,  0.87831213,  0.88225772,  0.6133479 ,  0.25438288],\n",
       "       [ 0.70978201,  0.03603945,  0.09643923,  0.44388884,  0.8624805 ,\n",
       "         0.45198811,  0.50665441,  0.72380933,  0.20903043,  0.33635637],\n",
       "       [ 0.83336628,  0.3274412 ,  0.93345078,  0.0840343 ,  0.28411972,\n",
       "         0.78082362,  0.33974629,  0.2466059 ,  0.41153243,  0.61772959],\n",
       "       [ 0.46673208,  0.36869651,  0.51794385,  0.74392312,  0.81041894,\n",
       "         0.99108213,  0.83448654,  0.14158009,  0.62232843,  0.20109928],\n",
       "       [ 0.32869277,  0.40581407,  0.76478634,  0.05637303,  0.28804032,\n",
       "         0.12474357,  0.88350763,  0.33066109,  0.99301764,  0.10080919],\n",
       "       [ 0.832262  ,  0.72988954,  0.9668223 ,  0.77446995,  0.89758952,\n",
       "         0.87587109,  0.26737801,  0.68780571,  0.90888125,  0.81643089],\n",
       "       [ 0.22748209,  0.99408391,  0.95243148,  0.87883525,  0.60509007,\n",
       "         0.37801129,  0.55669543,  0.07286532,  0.22277967,  0.74928235],\n",
       "       [ 0.9947066 ,  0.79406557,  0.41100997,  0.82731693,  0.41775361,\n",
       "         0.80264272,  0.54873063,  0.57764756,  0.26047039,  0.38019301],\n",
       "       [ 0.32372638,  0.48571148,  0.6327166 ,  0.80783921,  0.43090645,\n",
       "         0.784342  ,  0.53996846,  0.58755344,  0.19858777,  0.20699572],\n",
       "       [ 0.88788775,  0.40782821,  0.67718189,  0.87422168,  0.82325697,\n",
       "         0.67637781,  0.28524453,  0.39944324,  0.67368675,  0.33173229],\n",
       "       [ 0.6808002 ,  0.27646168,  0.4073165 ,  0.07993558,  0.03358152,\n",
       "         0.7360217 ,  0.72674165,  0.45895796,  0.43274146,  0.93418503],\n",
       "       [ 0.11924441,  0.4167399 ,  0.3756737 ,  0.38233454,  0.47288302,\n",
       "         0.8728641 ,  0.42275211,  0.55731344,  0.72356626,  0.18013438],\n",
       "       [ 0.25048683,  0.51902598,  0.48319862,  0.70673669,  0.09634105,\n",
       "         0.13126179,  0.97758126,  0.44058605,  0.27287356,  0.94609649],\n",
       "       [ 0.57083664,  0.65162448,  0.92360752,  0.28936731,  0.28153899,\n",
       "         0.28695356,  0.25543566,  0.95445451,  0.24076093,  0.93960668],\n",
       "       [ 0.20447168,  0.57776663,  0.41337692,  0.37081843,  0.04383483,\n",
       "         0.05336615,  0.17820829,  0.10045139,  0.56687753,  0.51258945],\n",
       "       [ 0.57440408,  0.59950157,  0.11804705,  0.83585455,  0.56573527,\n",
       "         0.89561862,  0.47304176,  0.57398825,  0.84908537,  0.05847679],\n",
       "       [ 0.99120802,  0.85706212,  0.62519632,  0.5125101 ,  0.67903859,\n",
       "         0.84303344,  0.93028937,  0.4922567 ,  0.85184749,  0.12087726],\n",
       "       [ 0.49986146,  0.22283991,  0.53750904,  0.11927654,  0.24710104,\n",
       "         0.97529774,  0.87969755,  0.66350395,  0.65103186,  0.83975626],\n",
       "       [ 0.14755766,  0.63795585,  0.30730577,  0.63456174,  0.99338797,\n",
       "         0.61131326,  0.00695353,  0.38333874,  0.86020685,  0.36269242],\n",
       "       [ 0.63963309,  0.64734416,  0.76524689,  0.86493211,  0.97958599,\n",
       "         0.37955095,  0.20906181,  0.05464276,  0.69043622,  0.60643327],\n",
       "       [ 0.18352532,  0.89209022,  0.9162812 ,  0.42896058,  0.83187464,\n",
       "         0.79497572,  0.25389035,  0.05531225,  0.99383711,  0.7292975 ],\n",
       "       [ 0.61130476,  0.00756266,  0.35601643,  0.93657194,  0.74618861,\n",
       "         0.30171778,  0.30364778,  0.29726664,  0.29909947,  0.69611691],\n",
       "       [ 0.8355799 ,  0.26209268,  0.10986966,  0.34572392,  0.45100274,\n",
       "         0.80290816,  0.76822422,  0.84341342,  0.007213  ,  0.21237167],\n",
       "       [ 0.82418796,  0.87017918,  0.46997451,  0.21993764,  0.60627821,\n",
       "         0.61646431,  0.8474197 ,  0.68731392,  0.02435665,  0.39683593],\n",
       "       [ 0.19175717,  0.88046179,  0.39399   ,  0.64790307,  0.28072098,\n",
       "         0.31089496,  0.27156356,  0.10911063,  0.88426345,  0.24836628],\n",
       "       [ 0.65720027,  0.41717512,  0.29946223,  0.5427451 ,  0.66953002,\n",
       "         0.1942725 ,  0.87227435,  0.15467087,  0.09285016,  0.05700562],\n",
       "       [ 0.68981445,  0.51597247,  0.03630807,  0.89250111,  0.37580239,\n",
       "         0.43635559,  0.71100764,  0.2466598 ,  0.11680918,  0.50196308],\n",
       "       [ 0.72501229,  0.99161158,  0.79661594,  0.03617571,  0.99772556,\n",
       "         0.40681332,  0.50026382,  0.85947231,  0.26085618,  0.64833542],\n",
       "       [ 0.76670194,  0.09279182,  0.20593052,  0.33267492,  0.96553465,\n",
       "         0.74125221,  0.08927144,  0.01595418,  0.12238597,  0.47984014],\n",
       "       [ 0.48660508,  0.261235  ,  0.41504825,  0.75276564,  0.18613315,\n",
       "         0.27719652,  0.17596195,  0.77064557,  0.49707224,  0.57024853],\n",
       "       [ 0.79386147,  0.92969043,  0.63770556,  0.64044402,  0.51695353,\n",
       "         0.66865671,  0.66445359,  0.7590265 ,  0.52028375,  0.25217989],\n",
       "       [ 0.37238448,  0.06234299,  0.81616095,  0.74555091,  0.22896447,\n",
       "         0.71725881,  0.29846558,  0.0465741 ,  0.19550473,  0.14802046],\n",
       "       [ 0.52234961,  0.48085355,  0.89796016,  0.75644411,  0.46695709,\n",
       "         0.01883634,  0.57081164,  0.98925113,  0.54479519,  0.82112224],\n",
       "       [ 0.90095184,  0.36876252,  0.09095826,  0.71830996,  0.02064745,\n",
       "         0.6313153 ,  0.21170098,  0.35122622,  0.14026599,  0.56059042],\n",
       "       [ 0.69190703,  0.43074774,  0.42573505,  0.44903637,  0.62391522,\n",
       "         0.78389642,  0.08606291,  0.77598823,  0.64115103,  0.06278035],\n",
       "       [ 0.99158596,  0.95586824,  0.12041031,  0.57910505,  0.94195963,\n",
       "         0.37450001,  0.98168322,  0.08989491,  0.54381117,  0.29295159],\n",
       "       [ 0.47569943,  0.2768198 ,  0.5025902 ,  0.80470635,  0.43780754,\n",
       "         0.43523906,  0.42306449,  0.86306288,  0.57620649,  0.58124015],\n",
       "       [ 0.5359621 ,  0.34001928,  0.42826672,  0.10844877,  0.55148376,\n",
       "         0.15177279,  0.009327  ,  0.50012245,  0.59556133,  0.66500833],\n",
       "       [ 0.2303976 ,  0.78663559,  0.22416074,  0.3870422 ,  0.67759913,\n",
       "         0.62500752,  0.78705518,  0.03999926,  0.6201858 ,  0.57078631],\n",
       "       [ 0.22378293,  0.14546762,  0.01179594,  0.18119293,  0.53303528,\n",
       "         0.30561647,  0.5138377 ,  0.40313591,  0.48546577,  0.48507018],\n",
       "       [ 0.2948763 ,  0.67239675,  0.741333  ,  0.18832598,  0.84856908,\n",
       "         0.07502932,  0.67447288,  0.66285208,  0.59831972,  0.58057489],\n",
       "       [ 0.84993222,  0.98396936,  0.45201259,  0.68455779,  0.71286622,\n",
       "         0.83143077,  0.80617185,  0.07492756,  0.15600302,  0.54293275],\n",
       "       [ 0.62084746,  0.60543897,  0.14255137,  0.48363325,  0.97385251,\n",
       "         0.97817163,  0.36949563,  0.28284231,  0.08528411,  0.37798119],\n",
       "       [ 0.05445548,  0.26274054,  0.60274896,  0.11067035,  0.31630018,\n",
       "         0.66912103,  0.64161922,  0.89505542,  0.82298492,  0.23848555],\n",
       "       [ 0.72425983,  0.51649277,  0.29771778,  0.22872173,  0.75782304,\n",
       "         0.42247348,  0.85392722,  0.67075106,  0.02228831,  0.1324597 ],\n",
       "       [ 0.82437405,  0.88480206,  0.26854815,  0.0058853 ,  0.99323841,\n",
       "         0.93518414,  0.11062091,  0.67838432,  0.14197892,  0.22366672],\n",
       "       [ 0.16818634,  0.64058734,  0.72990018,  0.53933157,  0.71344804,\n",
       "         0.46761938,  0.26192287,  0.81366482,  0.0971615 ,  0.68959025],\n",
       "       [ 0.42931668,  0.91517959,  0.5212031 ,  0.35769315,  0.61602226,\n",
       "         0.35807211,  0.01512461,  0.25449367,  0.27027186,  0.84361393],\n",
       "       [ 0.12711963,  0.98945353,  0.26743007,  0.44121037,  0.28743181,\n",
       "         0.10385881,  0.88023474,  0.57747811,  0.66705047,  0.66575997],\n",
       "       [ 0.55565048,  0.05122947,  0.19233997,  0.54979384,  0.27701542,\n",
       "         0.59402694,  0.8147785 ,  0.30148178,  0.39258193,  0.93894585],\n",
       "       [ 0.5399712 ,  0.18868109,  0.44666118,  0.70620158,  0.92899815,\n",
       "         0.22698097,  0.22681024,  0.92429046,  0.73527783,  0.89305717],\n",
       "       [ 0.3529232 ,  0.20414334,  0.96265606,  0.55986977,  0.00812148,\n",
       "         0.61212589,  0.75933473,  0.25378431,  0.55605329,  0.58925351],\n",
       "       [ 0.80871339,  0.53476014,  0.38809422,  0.705121  ,  0.76747526,\n",
       "         0.00560418,  0.34505674,  0.45481658,  0.51330929,  0.07008325],\n",
       "       [ 0.87752514,  0.39876876,  0.19542392,  0.53952361,  0.36118931,\n",
       "         0.05421146,  0.7129318 ,  0.39436751,  0.78832271,  0.01359238],\n",
       "       [ 0.32083111,  0.5670258 ,  0.87144081,  0.23745803,  0.97897583,\n",
       "         0.58807299,  0.98985821,  0.85533484,  0.35711504,  0.25240145],\n",
       "       [ 0.89977041,  0.3986988 ,  0.84005344,  0.83525008,  0.13082366,\n",
       "         0.30516601,  0.85029866,  0.9364551 ,  0.43102511,  0.97927765],\n",
       "       [ 0.6745634 ,  0.81006126,  0.77763698,  0.28824288,  0.26521207,\n",
       "         0.89637289,  0.65738559,  0.67053426,  0.28722393,  0.049602  ],\n",
       "       [ 0.38148865,  0.43363174,  0.44007784,  0.53074389,  0.27173166,\n",
       "         0.89020276,  0.41647952,  0.64775629,  0.39798072,  0.09004324],\n",
       "       [ 0.11758469,  0.70148504,  0.6335859 ,  0.70731903,  0.6738517 ,\n",
       "         0.82516564,  0.73870404,  0.19801996,  0.69575156,  0.50888204],\n",
       "       [ 0.40357025,  0.07536736,  0.93817212,  0.19675895,  0.12020034,\n",
       "         0.05236844,  0.95123825,  0.80132207,  0.41706038,  0.3006495 ],\n",
       "       [ 0.35988234,  0.2169683 ,  0.63928392,  0.65352106,  0.09723412,\n",
       "         0.93723274,  0.59239738,  0.2434401 ,  0.92423598,  0.10005761],\n",
       "       [ 0.02106506,  0.68960025,  0.8794919 ,  0.50769141,  0.80196359,\n",
       "         0.65918455,  0.44425182,  0.95278042,  0.93353986,  0.18945994],\n",
       "       [ 0.47437074,  0.74220047,  0.01364363,  0.63367853,  0.91255976,\n",
       "         0.46643462,  0.3313865 ,  0.42660887,  0.96681734,  0.37523781],\n",
       "       [ 0.404008  ,  0.71008305,  0.32213664,  0.49069801,  0.60395569,\n",
       "         0.53977819,  0.12524854,  0.17175123,  0.97548601,  0.46607221],\n",
       "       [ 0.16065429,  0.94892782,  0.79494304,  0.99324548,  0.70269159,\n",
       "         0.50663102,  0.96256114,  0.48849376,  0.09703336,  0.2895639 ],\n",
       "       [ 0.63731371,  0.66093651,  0.28008126,  0.69613648,  0.21669206,\n",
       "         0.89743304,  0.64108225,  0.89072577,  0.36640284,  0.92737808],\n",
       "       [ 0.04103121,  0.52472095,  0.11850997,  0.13496489,  0.63947069,\n",
       "         0.56152153,  0.66992572,  0.05621958,  0.72868389,  0.05085786],\n",
       "       [ 0.82888011,  0.05181506,  0.01608698,  0.44442872,  0.41134994,\n",
       "         0.32680995,  0.09193207,  0.42648324,  0.3600811 ,  0.80379633],\n",
       "       [ 0.73546623,  0.37900403,  0.02083058,  0.65087597,  0.73365542,\n",
       "         0.30254901,  0.67952783,  0.16748139,  0.17230111,  0.06838192],\n",
       "       [ 0.38479351,  0.78919711,  0.67319034,  0.4274404 ,  0.95625686,\n",
       "         0.73000789,  0.38289761,  0.38630826,  0.17831394,  0.67821025],\n",
       "       [ 0.64014468,  0.80302248,  0.71648218,  0.51835238,  0.45539908,\n",
       "         0.05348111,  0.00828946,  0.95147532,  0.21573047,  0.0578277 ],\n",
       "       [ 0.14498592,  0.87545341,  0.04356173,  0.7531578 ,  0.77346451,\n",
       "         0.50897411,  0.45653878,  0.7473736 ,  0.87742979,  0.88907884],\n",
       "       [ 0.54791212,  0.38758563,  0.50261533,  0.85231231,  0.67239008,\n",
       "         0.45488078,  0.71623051,  0.95773974,  0.15022765,  0.36076435],\n",
       "       [ 0.75193765,  0.73539763,  0.9632749 ,  0.06811173,  0.10700466,\n",
       "         0.57264533,  0.99834209,  0.77566423,  0.46192882,  0.81080961],\n",
       "       [ 0.27898075,  0.05998408,  0.98928075,  0.7391111 ,  0.81815496,\n",
       "         0.21978406,  0.60107238,  0.64143205,  0.38439776,  0.20607125],\n",
       "       [ 0.37597146,  0.53529   ,  0.7394096 ,  0.40955597,  0.63710477,\n",
       "         0.35860041,  0.75776232,  0.69780759,  0.66379155,  0.50062938],\n",
       "       [ 0.54607319,  0.15267584,  0.50658015,  0.8658735 ,  0.98700011,\n",
       "         0.31904415,  0.2176466 ,  0.95577915,  0.77734834,  0.62308056],\n",
       "       [ 0.64722413,  0.37507622,  0.35397137,  0.76705289,  0.44683421,\n",
       "         0.80960983,  0.70632766,  0.17147773,  0.5910514 ,  0.83627576],\n",
       "       [ 0.12919161,  0.981128  ,  0.20131392,  0.22888786,  0.98916765,\n",
       "         0.71022299,  0.65954364,  0.13512179,  0.98831445,  0.36619959],\n",
       "       [ 0.4374771 ,  0.61794248,  0.03371006,  0.5984514 ,  0.43438902,\n",
       "         0.92842423,  0.42576237,  0.86481518,  0.53648831,  0.01554244],\n",
       "       [ 0.40590663,  0.48125834,  0.3055758 ,  0.51220002,  0.6224763 ,\n",
       "         0.75261006,  0.61378596,  0.69614979,  0.63515533,  0.47368211],\n",
       "       [ 0.08715407,  0.36101296,  0.00369549,  0.7146988 ,  0.8088379 ,\n",
       "         0.69656022,  0.63000149,  0.6919524 ,  0.05579889,  0.95155761],\n",
       "       [ 0.66848281,  0.5385676 ,  0.23745489,  0.17451101,  0.05108718,\n",
       "         0.71214686,  0.01003364,  0.71759413,  0.25834813,  0.03654238],\n",
       "       [ 0.16790709,  0.33564155,  0.90444922,  0.65654835,  0.46150904,\n",
       "         0.04090879,  0.64152952,  0.90325884,  0.18773434,  0.85474836],\n",
       "       [ 0.49095935,  0.69672924,  0.59612226,  0.65413995,  0.2336794 ,\n",
       "         0.53268717,  0.51821521,  0.69721699,  0.03956121,  0.33052439],\n",
       "       [ 0.58721778,  0.15501562,  0.94570985,  0.79902504,  0.12892016,\n",
       "         0.85315844,  0.97184088,  0.04145829,  0.50353595,  0.54647331],\n",
       "       [ 0.7723955 ,  0.82086349,  0.05710733,  0.29569512,  0.95262516,\n",
       "         0.18218184,  0.15014208,  0.04937304,  0.99160092,  0.16361651],\n",
       "       [ 0.58390268,  0.99627426,  0.15433971,  0.42387922,  0.71146615,\n",
       "         0.04919436,  0.0997355 ,  0.93171538,  0.35893537,  0.94428316],\n",
       "       [ 0.80458814,  0.23752489,  0.26734887,  0.20626299,  0.72786272,\n",
       "         0.17203324,  0.11422339,  0.51037313,  0.57328062,  0.67694321],\n",
       "       [ 0.88843865,  0.62637454,  0.65059721,  0.69002317,  0.40583737,\n",
       "         0.14436987,  0.43360849,  0.75542636,  0.95480809,  0.11639529],\n",
       "       [ 0.12844475,  0.48306903,  0.12128574,  0.0817815 ,  0.03860564,\n",
       "         0.67173524,  0.35275264,  0.66386938,  0.62161129,  0.51456473],\n",
       "       [ 0.21128995,  0.26275642,  0.33863821,  0.20876897,  0.08241283,\n",
       "         0.01546269,  0.05643828,  0.51605521,  0.74420278,  0.00514806],\n",
       "       [ 0.33138235,  0.02272039,  0.34732521,  0.85580514,  0.20846698,\n",
       "         0.82718573,  0.68973524,  0.05993916,  0.18827103,  0.53723148],\n",
       "       [ 0.48841427,  0.80469854,  0.65297158,  0.12735208,  0.74283254,\n",
       "         0.61626127,  0.43999924,  0.20717717,  0.02494518,  0.83006864],\n",
       "       [ 0.66536289,  0.61581616,  0.90562115,  0.02666105,  0.0367095 ,\n",
       "         0.98722241,  0.98599836,  0.77254324,  0.52912738,  0.88868088],\n",
       "       [ 0.87165208,  0.43365109,  0.75096703,  0.79332116,  0.1584873 ,\n",
       "         0.82401545,  0.87533762,  0.42978392,  0.72008098,  0.93997507],\n",
       "       [ 0.08506684,  0.68059943,  0.26610444,  0.87221095,  0.55942442,\n",
       "         0.00229678,  0.13225654,  0.51623334,  0.0560573 ,  0.61143614],\n",
       "       [ 0.8813539 ,  0.73566514,  0.11689832,  0.35450312,  0.44782519,\n",
       "         0.4965548 ,  0.35778835,  0.10034807,  0.02037723,  0.93349218],\n",
       "       [ 0.70617741,  0.49582512,  0.29555927,  0.90531146,  0.87301478,\n",
       "         0.78830004,  0.83276347,  0.34347334,  0.12502148,  0.53438238]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-da50eb733ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "self.indices = np.asarray(range(data.shape[0]-self.timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.arange(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=a[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = b.reshape(5,20)\n",
    "c=c.reshape(5,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-151c63719b8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for j in range(20):\n",
    "        index = np.where(b[i] == c[i][j])\n",
    "        print(index[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19],\n",
       "       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "        37, 38, 39],\n",
       "       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56,\n",
       "        57, 58, 59],\n",
       "       [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76,\n",
       "        77, 78, 79],\n",
       "       [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96,\n",
       "        97, 98, 99]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20],\n",
       "       [ 21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,\n",
       "         34,  35,  36,  37,  38,  39,  40],\n",
       "       [ 41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
       "         54,  55,  56,  57,  58,  59,  60],\n",
       "       [ 61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
       "         74,  75,  76,  77,  78,  79,  80],\n",
       "       [ 81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
       "         94,  95,  96,  97,  98,  99, 100]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3081 originated -> 5234 anarchism\n",
      "3081 originated -> 12 as\n",
      "12 as -> 6 a\n",
      "12 as -> 3081 originated\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "Initialized\n",
      "Average loss at step  0 :  321.040008545\n",
      "Nearest to than: stockport, derelict, timed, grab, carbonate, parthia, discern, gustavo,\n",
      "Nearest to th: franchise, dispelling, endocytosis, cpsu, nikolay, gana, minorities, krzy,\n",
      "Nearest to up: deepwater, millard, antiochian, aforementioned, lillee, ballast, gauntlet, sore,\n",
      "Nearest to when: pity, cimmerians, hutterite, plebiscite, exploitable, epigrams, overlook, regnant,\n",
      "Nearest to war: fatale, musculus, cooler, mosfet, manometer, apple, holly, guilds,\n",
      "Nearest to often: slits, pericles, hummingbird, kaiju, ramifications, spices, wie, sip,\n",
      "Nearest to and: topics, jpeg, maximally, poulsen, gerard, depraved, edison, cosmologies,\n",
      "Nearest to been: palma, obstruction, slavs, cure, kankan, incarnations, differs, rolland,\n",
      "Nearest to its: killers, sadat, pn, probability, mania, cons, admit, triomphe,\n",
      "Nearest to such: londinium, vesta, marrakech, scored, kvac, tragically, irate, taxonomic,\n",
      "Nearest to most: rabbani, liberia, tavistock, hallstatt, baldassare, essene, merseburg, kubrick,\n",
      "Nearest to use: evolutionist, ethically, jerzy, fingerprinting, gules, cante, adsl, dataset,\n",
      "Nearest to or: munich, crichton, measurement, mutation, tuners, backlight, goths, calcio,\n",
      "Nearest to only: scotsman, fantasia, bled, saddles, badlands, perlman, homeopathy, intermission,\n",
      "Nearest to however: devries, posthumus, juggernaut, everquest, access, crassus, finalised, drift,\n",
      "Nearest to their: crusader, latour, eldridge, chalcedonian, grateful, austria, rida, pomeroy,\n",
      "Average loss at step  2000 :  113.099180621\n",
      "Average loss at step  4000 :  53.2272577314\n",
      "Average loss at step  6000 :  33.1277453343\n",
      "Average loss at step  8000 :  23.249935307\n",
      "Average loss at step  10000 :  17.8664979815\n",
      "Nearest to than: or, grab, opening, gustavo, and, richmond, nine, sponsor,\n",
      "Nearest to th: zero, stories, contract, archie, dispelling, importance, one, six,\n",
      "Nearest to up: deepwater, forbade, eight, rise, ep, bc, millard, trench,\n",
      "Nearest to when: economists, addition, ruins, basins, plebiscite, served, standard, exist,\n",
      "Nearest to war: cooler, apple, frequency, crowd, sx, gland, fellow, sarah,\n",
      "Nearest to often: portal, qutb, islamic, guidelines, suffering, iran, lie, liberties,\n",
      "Nearest to and: in, of, UNK, or, for, the, hydrate, gland,\n",
      "Nearest to been: chosen, cure, obstruction, hoare, governor, to, hall, publicized,\n",
      "Nearest to its: probability, the, mathbf, wallpaper, citing, basin, tyrannical, treated,\n",
      "Nearest to such: scored, vesta, reginae, parts, ainu, sy, irate, agave,\n",
      "Nearest to most: kubrick, works, tavistock, increases, liberia, stigma, consequent, fascist,\n",
      "Nearest to use: order, second, politics, beginning, aberdeenshire, ignosticism, biotechnology, ic,\n",
      "Nearest to or: and, naci, gland, backlight, trilobites, than, insurance, bckgr,\n",
      "Nearest to only: gch, in, protestant, offense, accidental, sleeping, california, mya,\n",
      "Nearest to however: november, coasts, access, efficiency, mind, basins, health, drift,\n",
      "Nearest to their: the, austria, mathbf, molecules, holy, underlying, displaced, reviled,\n",
      "Average loss at step  12000 :  14.1424553298\n",
      "Average loss at step  14000 :  12.2447076499\n",
      "Average loss at step  16000 :  10.0634448923\n",
      "Average loss at step  18000 :  8.38748592794\n",
      "Average loss at step  20000 :  8.06603139508\n",
      "Nearest to than: or, and, violin, timed, sponsor, nine, devastated, grab,\n",
      "Nearest to th: franchise, contract, eight, six, dispelling, stories, seljuk, zero,\n",
      "Nearest to up: deepwater, forbade, dasyprocta, millard, rise, old, lillee, ep,\n",
      "Nearest to when: pity, agouti, and, standard, remember, economists, basins, is,\n",
      "Nearest to war: cooler, apple, frequency, disparity, fellow, crowd, sarah, dharma,\n",
      "Nearest to often: pericles, operatorname, not, ps, dasyprocta, guidelines, aloes, hearings,\n",
      "Nearest to and: in, or, dasyprocta, of, agouti, for, operatorname, altenberg,\n",
      "Nearest to been: were, by, publicized, chosen, cure, obstruction, hoare, branched,\n",
      "Nearest to its: the, his, probability, mathbf, citing, tyrannical, diversified, basin,\n",
      "Nearest to such: scored, ainu, vesta, reginae, marrakech, irate, sy, stress,\n",
      "Nearest to most: kubrick, works, tavistock, increases, agouti, liberia, gollancz, strenuous,\n",
      "Nearest to use: albuquerque, order, agouti, dasyprocta, politics, second, beginning, aberdeenshire,\n",
      "Nearest to or: and, UNK, agouti, operatorname, than, eight, gland, dasyprocta,\n",
      "Nearest to only: scotsman, gch, accidental, protestant, offense, and, enlarged, blinded,\n",
      "Nearest to however: but, ulyanov, and, wattle, lovell, mind, asymmetric, efficiency,\n",
      "Nearest to their: the, his, austria, operatorname, mathbf, displaced, equipped, tend,\n",
      "Average loss at step  22000 :  7.04489651477\n",
      "Average loss at step  24000 :  6.76158836293\n",
      "Average loss at step  26000 :  6.71481688547\n",
      "Average loss at step  28000 :  6.36287568367\n",
      "Average loss at step  30000 :  5.93195614648\n",
      "Nearest to than: or, and, sponsor, parthia, gustavo, with, violin, devastated,\n",
      "Nearest to th: franchise, eight, six, contract, nine, seljuk, seven, dispelling,\n",
      "Nearest to up: forbade, gauntlet, dasyprocta, deepwater, rise, millard, merwara, gnat,\n",
      "Nearest to when: and, after, three, bpp, agouti, is, pity, as,\n",
      "Nearest to war: cooler, apple, frequency, disparity, fellow, sarah, crowd, jains,\n",
      "Nearest to often: not, pericles, operatorname, ps, dasyprocta, it, guidelines, also,\n",
      "Nearest to and: or, agouti, operatorname, dasyprocta, in, birkenau, for, of,\n",
      "Nearest to been: by, were, publicized, cure, chosen, hoare, pitching, branched,\n",
      "Nearest to its: the, his, their, probability, mathbf, citing, diversified, a,\n",
      "Nearest to such: scored, ainu, vesta, reginae, dasyprocta, crops, marrakech, stress,\n",
      "Nearest to most: works, kubrick, baldassare, increases, tavistock, liberia, rabbani, wilcox,\n",
      "Nearest to use: albuquerque, order, agouti, beginning, ize, ic, dasyprocta, biotechnology,\n",
      "Nearest to or: and, agouti, operatorname, dasyprocta, than, eight, the, circ,\n",
      "Nearest to only: scotsman, gch, in, accidental, enlarged, offense, noble, protestant,\n",
      "Nearest to however: but, aediles, and, ulyanov, ajanta, wattle, although, argentine,\n",
      "Nearest to their: the, his, its, operatorname, equipped, mathbf, ghats, her,\n",
      "Average loss at step  32000 :  5.94241716242\n",
      "Average loss at step  34000 :  5.74435634804\n",
      "Average loss at step  36000 :  5.76713713336\n",
      "Average loss at step  38000 :  5.5238444109\n",
      "Average loss at step  40000 :  5.26633533472\n",
      "Nearest to than: or, and, sponsor, gustavo, nine, discern, parthia, devastated,\n",
      "Nearest to th: eight, seven, six, franchise, zero, nine, seljuk, four,\n",
      "Nearest to up: forbade, gauntlet, deepwater, rise, dasyprocta, him, millard, gnat,\n",
      "Nearest to when: after, and, tdma, agouti, pity, would, kickstart, if,\n",
      "Nearest to war: cooler, apple, fellow, frequency, disparity, jains, sarah, dharma,\n",
      "Nearest to often: not, also, pericles, sometimes, still, it, operatorname, ps,\n",
      "Nearest to and: or, agouti, dasyprocta, operatorname, in, birkenau, zero, but,\n",
      "Nearest to been: by, were, publicized, be, cure, chosen, was, pitching,\n",
      "Nearest to its: their, the, his, citing, any, diversified, probability, mathbf,\n",
      "Nearest to such: ainu, scored, vesta, well, kvac, albury, photosynthesis, plates,\n",
      "Nearest to most: works, kubrick, baldassare, veneration, agouti, increases, liberia, centralized,\n",
      "Nearest to use: albuquerque, order, agouti, dasyprocta, ize, beginning, ic, hbf,\n",
      "Nearest to or: and, agouti, operatorname, dasyprocta, aveiro, eight, zero, gland,\n",
      "Nearest to only: scotsman, gch, in, bp, offense, enlarged, situation, ali,\n",
      "Nearest to however: but, aediles, although, lovell, ajanta, ulyanov, and, wattle,\n",
      "Nearest to their: his, the, its, her, equipped, operatorname, mathbf, tend,\n",
      "Average loss at step  42000 :  5.41682723284\n",
      "Average loss at step  44000 :  5.24740532255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  46000 :  5.22797923934\n",
      "Average loss at step  48000 :  5.23233724272\n",
      "Average loss at step  50000 :  4.99865053856\n",
      "Nearest to than: or, and, gustavo, sponsor, discern, oriental, parthia, devastated,\n",
      "Nearest to th: eight, seven, seljuk, four, franchise, six, three, nine,\n",
      "Nearest to up: gauntlet, forbade, him, dasyprocta, rise, deepwater, gnat, them,\n",
      "Nearest to when: after, and, six, if, while, three, seven, agouti,\n",
      "Nearest to war: cooler, apple, fatale, disparity, fellow, frequency, jains, computational,\n",
      "Nearest to often: not, also, still, sometimes, pericles, it, operatorname, there,\n",
      "Nearest to and: or, operatorname, dasyprocta, eight, agouti, but, six, birkenau,\n",
      "Nearest to been: by, were, be, publicized, was, cure, are, pitching,\n",
      "Nearest to its: their, the, his, citing, probability, mathbf, any, diversified,\n",
      "Nearest to such: ainu, scored, well, vesta, these, conform, kvac, albury,\n",
      "Nearest to most: works, more, veneration, baldassare, kubrick, liberia, centralized, wilcox,\n",
      "Nearest to use: albuquerque, order, agouti, dasyprocta, ize, beginning, ic, centennial,\n",
      "Nearest to or: and, agouti, operatorname, eight, than, two, three, four,\n",
      "Nearest to only: banjos, scotsman, queuing, username, three, gch, bp, ciphers,\n",
      "Nearest to however: but, aediles, although, and, four, then, ulyanov, wattle,\n",
      "Nearest to their: his, its, the, her, operatorname, equipped, dasyprocta, mathbf,\n",
      "Average loss at step  52000 :  5.04674752796\n",
      "Average loss at step  54000 :  5.20182010794\n",
      "Average loss at step  56000 :  5.05778836894\n",
      "Average loss at step  58000 :  5.06118032444\n",
      "Average loss at step  60000 :  4.94612151444\n",
      "Nearest to than: or, gustavo, and, sponsor, oriental, parthia, discern, devastated,\n",
      "Nearest to th: eight, six, seljuk, four, ursus, seven, franchise, nine,\n",
      "Nearest to up: forbade, him, gauntlet, dasyprocta, them, rise, gnat, deepwater,\n",
      "Nearest to when: after, if, while, five, six, agouti, seven, bckgr,\n",
      "Nearest to war: cooler, fatale, apple, fellow, musculus, frequency, disparity, holly,\n",
      "Nearest to often: also, still, not, sometimes, there, pericles, dasyprocta, operatorname,\n",
      "Nearest to and: or, ursus, operatorname, agouti, birkenau, dasyprocta, in, but,\n",
      "Nearest to been: be, by, were, was, publicized, had, pitching, microsite,\n",
      "Nearest to its: their, his, the, citing, any, diversified, her, probability,\n",
      "Nearest to such: ainu, well, scored, known, these, thurston, vesta, reginae,\n",
      "Nearest to most: more, works, baldassare, veneration, liberia, centralized, kubrick, agouti,\n",
      "Nearest to use: albuquerque, order, ize, dasyprocta, agouti, beginning, ic, centennial,\n",
      "Nearest to or: and, agouti, ursus, operatorname, than, dasyprocta, aveiro, eight,\n",
      "Nearest to only: banjos, scotsman, michelob, queuing, but, username, cleaned, ciphers,\n",
      "Nearest to however: but, although, ursus, aediles, and, then, ulyanov, wattle,\n",
      "Nearest to their: his, its, the, her, operatorname, these, dasyprocta, some,\n",
      "Average loss at step  62000 :  4.98269582808\n",
      "Average loss at step  64000 :  4.83367885917\n",
      "Average loss at step  66000 :  4.60011399674\n",
      "Average loss at step  68000 :  5.00795728993\n",
      "Average loss at step  70000 :  4.90758545017\n",
      "Nearest to than: or, and, gustavo, sponsor, but, mort, parthia, devastated,\n",
      "Nearest to th: eight, six, seljuk, seven, nine, ursus, franchise, worsened,\n",
      "Nearest to up: him, gauntlet, forbade, them, dasyprocta, back, gnat, rise,\n",
      "Nearest to when: after, if, while, agouti, six, was, thaler, bckgr,\n",
      "Nearest to war: cooler, fatale, apple, fellow, disparity, musculus, frequency, holly,\n",
      "Nearest to often: still, sometimes, also, not, there, which, guidelines, pericles,\n",
      "Nearest to and: or, ursus, agouti, operatorname, dasyprocta, but, thaler, seven,\n",
      "Nearest to been: be, were, was, by, are, publicized, had, pitching,\n",
      "Nearest to its: their, his, the, citing, any, her, probability, mathbf,\n",
      "Nearest to such: ainu, well, these, known, scored, many, vesta, conform,\n",
      "Nearest to most: more, works, veneration, baldassare, some, use, liberia, centralized,\n",
      "Nearest to use: albuquerque, order, dasyprocta, ize, agouti, ic, beginning, dinar,\n",
      "Nearest to or: and, agouti, ursus, operatorname, than, dasyprocta, thaler, UNK,\n",
      "Nearest to only: banjos, dinar, but, queuing, michelob, scotsman, thaler, username,\n",
      "Nearest to however: but, although, ursus, and, microcebus, hagbard, aediles, dinar,\n",
      "Nearest to their: its, his, her, the, thaler, some, these, dasyprocta,\n",
      "Average loss at step  72000 :  4.74118346584\n",
      "Average loss at step  74000 :  4.80839243197\n",
      "Average loss at step  76000 :  4.74009124887\n",
      "Average loss at step  78000 :  4.79055942166\n",
      "Average loss at step  80000 :  4.77838619816\n",
      "Nearest to than: or, and, gustavo, but, sponsor, mort, discern, parthia,\n",
      "Nearest to th: eight, six, seven, seljuk, four, ursus, worsened, nine,\n",
      "Nearest to up: him, gauntlet, them, forbade, out, back, rise, gnat,\n",
      "Nearest to when: after, while, if, seven, six, agouti, thaler, bckgr,\n",
      "Nearest to war: cooler, fatale, apple, fellow, disparity, musculus, holly, computational,\n",
      "Nearest to often: sometimes, still, also, not, there, commonly, which, generally,\n",
      "Nearest to and: or, agouti, ursus, operatorname, thaler, dasyprocta, but, birkenau,\n",
      "Nearest to been: be, by, were, was, had, ever, nine, erotic,\n",
      "Nearest to its: their, his, the, citing, any, her, probability, dasyprocta,\n",
      "Nearest to such: these, well, ainu, known, many, conform, some, scored,\n",
      "Nearest to most: more, some, works, use, veneration, baldassare, centralized, agouti,\n",
      "Nearest to use: albuquerque, order, ic, ize, dasyprocta, iit, agouti, centennial,\n",
      "Nearest to or: and, agouti, than, ursus, operatorname, dasyprocta, circ, aveiro,\n",
      "Nearest to only: banjos, dinar, queuing, but, scotsman, username, concourse, michelob,\n",
      "Nearest to however: but, although, ursus, microcebus, that, hagbard, dinar, aediles,\n",
      "Nearest to their: its, his, her, the, thaler, dasyprocta, some, operatorname,\n",
      "Average loss at step  82000 :  4.77381898522\n",
      "Average loss at step  84000 :  4.76161619186\n",
      "Average loss at step  86000 :  4.76785810959\n",
      "Average loss at step  88000 :  4.7440997386\n",
      "Average loss at step  90000 :  4.74598345399\n",
      "Nearest to than: or, but, gustavo, and, sponsor, mort, parthia, devastated,\n",
      "Nearest to th: eight, six, seven, nine, seljuk, ursus, archie, worsened,\n",
      "Nearest to up: him, them, out, gauntlet, forbade, back, rise, dasyprocta,\n",
      "Nearest to when: after, while, if, until, where, six, although, seven,\n",
      "Nearest to war: cooler, fatale, apple, fellow, holly, disparity, argentina, jains,\n",
      "Nearest to often: sometimes, still, also, not, there, commonly, generally, usually,\n",
      "Nearest to and: or, but, ursus, operatorname, agouti, thaler, while, dasyprocta,\n",
      "Nearest to been: be, by, was, were, are, become, ever, erotic,\n",
      "Nearest to its: their, the, his, citing, any, her, peacocks, thaler,\n",
      "Nearest to such: these, well, ainu, known, some, many, conform, scored,\n",
      "Nearest to most: more, some, use, works, veneration, centralized, obtaining, agouti,\n",
      "Nearest to use: albuquerque, ize, agouti, dasyprocta, iit, ic, jati, order,\n",
      "Nearest to or: and, agouti, ursus, than, operatorname, dasyprocta, thaler, but,\n",
      "Nearest to only: dinar, banjos, but, queuing, cleaned, peacocks, thaler, michelob,\n",
      "Nearest to however: but, although, ursus, microcebus, that, and, dinar, hagbard,\n",
      "Nearest to their: its, his, the, her, some, thaler, these, creative,\n",
      "Average loss at step  92000 :  4.68781727529\n",
      "Average loss at step  94000 :  4.72443449211\n",
      "Average loss at step  96000 :  4.6800726943\n",
      "Average loss at step  98000 :  4.61120519233\n",
      "Average loss at step  100000 :  4.6913292408\n",
      "Nearest to than: or, and, but, gustavo, sponsor, mort, devastated, seven,\n",
      "Nearest to th: eight, six, seljuk, seven, nine, ursus, worsened, archie,\n",
      "Nearest to up: him, out, them, back, gauntlet, forbade, rise, tendency,\n",
      "Nearest to when: if, while, after, until, where, although, seven, bckgr,\n",
      "Nearest to war: cooler, fatale, disparity, argentina, apple, fellow, writ, holly,\n",
      "Nearest to often: sometimes, still, also, commonly, there, usually, not, generally,\n",
      "Nearest to and: or, but, agouti, operatorname, dasyprocta, ursus, birkenau, thaler,\n",
      "Nearest to been: be, by, was, were, become, ever, erotic, had,\n",
      "Nearest to its: their, his, the, citing, her, any, peacocks, yin,\n",
      "Nearest to such: these, well, known, ainu, many, some, firing, kvac,\n",
      "Nearest to most: more, some, many, use, works, veneration, centralized, agouti,\n",
      "Nearest to use: albuquerque, dasyprocta, order, ic, iit, agouti, ize, most,\n",
      "Nearest to or: and, than, agouti, ursus, dasyprocta, but, operatorname, circ,\n",
      "Nearest to only: dinar, banjos, queuing, cebus, michelob, peacocks, roughly, thaler,\n",
      "Nearest to however: but, although, ursus, microcebus, that, dinar, hagbard, while,\n",
      "Nearest to their: its, his, her, the, these, some, thaler, creative,\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Basic word2vec example.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Download the data.\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "\n",
    "# pylint: disable=redefined-outer-name\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  local_filename = os.path.join(gettempdir(), filename)\n",
    "  if not os.path.exists(local_filename):\n",
    "    local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
    "                                                   local_filename)\n",
    "  statinfo = os.stat(local_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception('Failed to verify ' + local_filename +\n",
    "                    '. Can you get to it with a browser?')\n",
    "  return local_filename\n",
    "\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size', len(vocabulary))\n",
    "\n",
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "  \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    index = dictionary.get(word, 0)\n",
    "    if index == 0:  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "# Filling 4 global variables:\n",
    "# data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "#   This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to their codes(integers)\n",
    "# reverse_dictionary - maps codes(integers) to words(strings)\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                            vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  if data_index + span > len(data):\n",
    "    data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips):\n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    words_to_use = random.sample(context_words, num_skips)\n",
    "    for j, context_word in enumerate(words_to_use):\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "    if data_index == len(data):\n",
    "      buffer[:] = data[:span]\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  # Explanation of the meaning of NCE loss:\n",
    "  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "  print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
